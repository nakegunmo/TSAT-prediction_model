{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 環境セットアップ（Google Driveマウント＋ライブラリインポート）\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    print(\"Drive mount skipped\")\n",
    "import math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Step 2: ファイルパス定義\n",
    "# TRAIN_PATH = \"/content/drive/MyDrive/TSAT/BTC_5min/BTC_full_5min_Train.csv\"\n",
    "# VALID_PATH = \"/content/drive/MyDrive/TSAT/BTC_5min/BTC_full_5min_Valid.csv\"\n",
    "\n",
    "TRAIN_PATH = \"/home/nagumo/TSAT/BTC_5min/BTC_full_5min_Train.csv\"\n",
    "VALID_PATH = \"/home/nagumo/TSAT/BTC_5min/BTC_full_5min_Valid.csv\"\n",
    "\n",
    "# Step 3: 再現性確保のためのシード固定\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 4: データ読み込み＆補間関数定義\n",
    "def load_and_interpolate(path):\n",
    "    df = pd.read_csv(path, parse_dates=['date']).set_index('date').sort_index()\n",
    "    idx = pd.date_range(df.index.min(), df.index.max(), freq='5T')\n",
    "    return df.reindex(idx).interpolate(method='time')\n",
    "\n",
    "# Step 5: データ読み込み実行＋期間フィルタリング\n",
    "df_train = load_and_interpolate(TRAIN_PATH)\n",
    "df_valid = load_and_interpolate(VALID_PATH)\n",
    "df_train = df_train[df_train.index >= '2020-01-01']\n",
    "print(\"Train:\", df_train.index.min(), \"to\", df_train.index.max())\n",
    "print(\"Valid:\", df_valid.index.min(), \"to\", df_valid.index.max())\n",
    "\n",
    "# Step 5: ハードラベル生成 (前後 k=6)\n",
    "def make_hard_labels(lows, highs, k=6):\n",
    "    n = len(lows)\n",
    "    lbl = np.zeros(n, dtype=int)\n",
    "    for t in range(k, n-k):\n",
    "        if lows[t] < lows[t-k:t].min() and lows[t] < lows[t+1:t+k+1].min():\n",
    "            lbl[t] = 1\n",
    "        elif highs[t] > highs[t-k:t].max() and highs[t] > highs[t+1:t+k+1].max():\n",
    "            lbl[t] = 2\n",
    "    return lbl\n",
    "\n",
    "lows_train, highs_train = df_train['low'].values, df_train['high'].values\n",
    "lows_val, highs_val     = df_valid['low'].values, df_valid['high'].values\n",
    "hard_train = make_hard_labels(lows_train, highs_train, k=6)\n",
    "hard_val   = make_hard_labels(lows_val, highs_val, k=6)\n",
    "df_train['hard_label'] = hard_train\n",
    "df_valid['hard_label'] = hard_val\n",
    "\n",
    "# Step 6: スコアベースソフトラベリング関数\n",
    "def compute_scores(lows, highs, k=6):\n",
    "    n = len(lows)\n",
    "    s_min = np.zeros(n)\n",
    "    s_max = np.zeros(n)\n",
    "    for t in range(k, n-k):\n",
    "        # 極小度\n",
    "        s_min[t] = 100*np.mean((lows[t-k:t] - lows[t]) / lows[t-k:t] +\n",
    "                           (lows[t+1:t+k+1] - lows[t]) / lows[t+1:t+k+1])\n",
    "        # 極大度\n",
    "        s_max[t] = 100*np.mean((highs[t] - highs[t-k:t]) / highs[t-k:t] +\n",
    "                           (highs[t] - highs[t+1:t+k+1]) / highs[t+1:t+k+1])\n",
    "    return s_min, s_max\n",
    "\n",
    "smin_train, smax_train = compute_scores(lows_train, highs_train, k=6)\n",
    "smin_val, smax_val     = compute_scores(lows_val, highs_val, k=6)\n",
    "\n",
    "def soft_label_from_scores(smin, smax):\n",
    "    # baseline for Other = 1.0\n",
    "    # logits = np.stack([np.ones_like(smin), smin, smax], axis=1)\n",
    "    logits = np.stack([1.5-np.maximum(smin, smax), smin, smax], axis=1) # other をsmin-smax\n",
    "    exp = np.exp(logits)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "soft_train = soft_label_from_scores(smin_train, smax_train)\n",
    "soft_val   = soft_label_from_scores(smin_val, smax_val)\n",
    "df_train['soft_label'] = list(soft_train)\n",
    "df_valid['soft_label'] = list(soft_val)\n",
    "\n",
    "# ソフトラベリングの結果を確認\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- ソフトラベルを numpy array に展開 ---\n",
    "soft_array = np.array(df_train['soft_label'].tolist())\n",
    "soft_other = soft_array[:, 0]\n",
    "soft_min   = soft_array[:, 1]\n",
    "soft_max   = soft_array[:, 2]\n",
    "\n",
    "# --- タイムインデックスと価格 ---\n",
    "times       = df_train.index\n",
    "low_series  = df_train['low']\n",
    "high_series = df_train['high']\n",
    "close_series = df_train['close']\n",
    "\n",
    "# --- プロット範囲を 2021-01-01 ～ 2021-01-31 に限定 ---\n",
    "start, end = pd.to_datetime(\"2021-11-01-12:00:00\"), pd.to_datetime(\"2021-11-01-23:00:00\")\n",
    "mask = (times >= start) & (times <= end)\n",
    "\n",
    "# --- 抽出 ---\n",
    "times_f        = times[mask]\n",
    "low_f          = low_series[mask]\n",
    "high_f         = high_series[mask]\n",
    "close_f       = close_series[mask]\n",
    "soft_min_f     = soft_min[mask]\n",
    "soft_max_f     = soft_max[mask]\n",
    "soft_other_f   = soft_other[mask]\n",
    "\n",
    "# --- プロット1: Low Price vs Soft Min Score ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "ax1.plot(times_f, low_f, label='Low Price', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(times_f, soft_min_f, label='Soft Min Score', color='tab:orange', alpha=0.7)\n",
    "ax2.set_ylim(0,1)\n",
    "ax1.set_title('2021-01 Low Price vs Soft Min Score')\n",
    "ax1.set_ylabel('Low Price')\n",
    "ax2.set_ylabel('Soft Min Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- プロット2: High Price vs Soft Max Score ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "ax1.plot(times_f, high_f, label='High Price', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(times_f, soft_max_f, label='Soft Max Score', color='tab:green', alpha=0.7)\n",
    "ax2.set_ylim(0,1)\n",
    "ax1.set_title('2021-01 High Price vs Soft Max Score')\n",
    "ax1.set_ylabel('High Price')\n",
    "ax2.set_ylabel('Soft Max Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- プロット3: Close vs Soft Other Score ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "ax1.plot(times_f, close_f, label='Close Price', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(times_f, soft_min_f, label='Soft Min Score', color='tab:orange', alpha=0.7)\n",
    "ax2.plot(times_f, soft_max_f,label='Soft Max Score', color='tab:green', alpha=0.7)\n",
    "ax2.plot(times_f, soft_other_f, label='Soft Other Score', color='tab:red', alpha=0.7)\n",
    "ax2.set_ylim(0,1)\n",
    "ax1.set_title('2021-01 Close Price vs Soft Other Score')\n",
    "ax1.set_ylabel('Close Price')\n",
    "ax2.set_ylabel('Soft Other Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 7: シーケンス特徴量 & ラベル生成 (SEQ_LEN=288)\n",
    "SEQ_LEN = 288\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm # tqdmライブラリをインポート (通常はスクリプトの先頭(Step1等)で一度だけインポートします)\n",
    "\n",
    "output_dir = \"evaluation_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_dataset(df, hard_lbl, soft_lbl, description=\"Creating dataset\"):\n",
    "    \"\"\"\n",
    "    DataFrameからシーケンス特徴量とラベルを生成します。\n",
    "    処理の進捗はtqdmプログレスバーで表示されます。\n",
    "a\n",
    "    Args:\n",
    "        df (pd.DataFrame): 入力DataFrame (価格データなどを含む)\n",
    "        hard_lbl (np.array): ハードラベル配列\n",
    "        soft_lbl (np.array): ソフトラベル配列\n",
    "        description (str, optional): tqdmプログレスバーに表示する説明文. Defaults to \"Creating dataset\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: X, y_hard, y_soft, y_reg, t_indices のnumpy配列を含むタプル\n",
    "    \"\"\"\n",
    "    X, y_hard, y_soft, y_reg, t_indices = [], [], [], [], []\n",
    "    \n",
    "    # DataFrameの行数に基づいてイテレーション。tqdmで進捗を表示。\n",
    "    # unit=\"sequences\" は進捗バーの単位がシーケンスであることを示す。\n",
    "    # leave=True (デフォルト) はループ終了後もプログレスバーを残す。\n",
    "    for t in tqdm(range(SEQ_LEN-1, len(df)-6), desc=description, unit=\"sequences\", leave=True):\n",
    "        win = df.iloc[t-SEQ_LEN+1:t+1]\n",
    "        \n",
    "        # --- 特徴量生成 ---\n",
    "        c0 = win['close'].iloc[-1] \n",
    "        v0 = win['volume'].iloc[-1]\n",
    "\n",
    "        norm_close = win['close'] / c0\n",
    "        norm_high = win['high'] / c0\n",
    "        norm_low = win['low'] / c0\n",
    "        \n",
    "        if v0 != 0:\n",
    "            normalized_volume = win['volume'] / v0\n",
    "        else:\n",
    "            normalized_volume = np.zeros_like(win['volume'], dtype=float) \n",
    "        \n",
    "        feats = np.stack([norm_close,\n",
    "                          norm_high,\n",
    "                          norm_low,\n",
    "                          normalized_volume], axis=1)\n",
    "        X.append(feats)\n",
    "        \n",
    "        # --- ラベル生成 ---\n",
    "        y_hard.append(hard_lbl[t])\n",
    "        y_soft.append(soft_lbl[t])\n",
    "        \n",
    "        if t + 1 < len(df): \n",
    "            c_plus_1 = df['close'].iloc[t+1]\n",
    "            if c0 != 0:\n",
    "                reg_target = c_plus_1 / c0\n",
    "            else:\n",
    "                reg_target = 1.0 \n",
    "        else:\n",
    "            reg_target = 1.0\n",
    "        y_reg.append(reg_target)\n",
    "        \n",
    "        t_indices.append(t) \n",
    "        \n",
    "    return np.array(X, dtype=np.float32), \\\n",
    "           np.array(y_hard), \\\n",
    "           np.array(y_soft, dtype=np.float32), \\\n",
    "           np.array(y_reg, dtype=np.float32), \\\n",
    "           np.array(t_indices)\n",
    "\n",
    "# --- データセット作成実行 ---\n",
    "# make_dataset呼び出し時に、tqdmプログレスバー用の説明文を指定\n",
    "print(\"--- Training data creation ---\")\n",
    "X_tr, y_tr_hard, y_tr_soft, y_tr_reg, t_tr_indices = make_dataset(\n",
    "    df_train, hard_train, soft_train, description=\"Processing df_train\"\n",
    ")\n",
    "print(\"\\n--- Validation/Test data creation ---\")\n",
    "X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices = make_dataset(\n",
    "    df_valid, hard_val, soft_val, description=\"Processing df_valid\"\n",
    ")\n",
    "print(\"\") # tqdmの表示とprint文の間にスペースを空ける\n",
    "\n",
    "# --- 検証データとテストデータの分割 ---\n",
    "if len(X_val_all) > 1:\n",
    "    X_val, X_test, \\\n",
    "    y_val_hard, y_test_hard, \\\n",
    "    y_val_soft, y_test_soft, \\\n",
    "    y_val_reg, y_test_reg, \\\n",
    "    t_val_indices, t_test_indices = train_test_split(\n",
    "        X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices,\n",
    "        test_size=0.5, random_state=SEED, shuffle=True # SEEDは事前に定義されている前提\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Not enough data in df_valid to split into validation and test sets after make_dataset. Using all for validation and test if applicable.\")\n",
    "    X_val, y_val_hard, y_val_soft, y_val_reg, t_val_indices = X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices\n",
    "    X_test, y_test_hard, y_test_soft, y_test_reg, t_test_indices = X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices\n",
    "\n",
    "# --- データ形状の確認表示 ---\n",
    "print(f\"X_tr shape: {X_tr.shape}, y_tr_reg shape: {y_tr_reg.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape if len(X_val)>0 else 'N/A'}, y_val_reg shape: {y_val_reg.shape if len(X_val)>0 else 'N/A'}\")\n",
    "print(f\"X_test shape: {X_test.shape if len(X_test)>0 else 'N/A'}, y_test_reg shape: {y_test_reg.shape if len(X_test)>0 else 'N/A'}\")\n",
    "\n",
    "# Step 8: Dataset & DataLoader\n",
    "class BTCSeqDataset(Dataset):\n",
    "    def __init__(self, X, y_hard, y_soft, y_reg, t_indices): # y_reg を追加\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y_hard = torch.from_numpy(y_hard)\n",
    "        self.y_soft = torch.from_numpy(y_soft)\n",
    "        self.y_reg = torch.from_numpy(y_reg) # y_reg を torch tensor に変換\n",
    "        self.t_indices = torch.from_numpy(t_indices)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y_hard[i], self.y_soft[i], self.y_reg[i], self.t_indices[i] # y_reg を返す\n",
    "\n",
    "BATCH = 64\n",
    "dl_tr = DataLoader(BTCSeqDataset(X_tr, y_tr_hard, y_tr_soft, y_tr_reg, t_tr_indices), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "if len(X_val) > 0 :\n",
    "    dl_val = DataLoader(BTCSeqDataset(X_val, y_val_hard, y_val_soft, y_val_reg, t_val_indices), batch_size=BATCH)\n",
    "else:\n",
    "    dl_val = None\n",
    "    print(\"Validation DataLoader (dl_val) is not created due to empty X_val.\")\n",
    "\n",
    "if len(X_test) > 0:\n",
    "    dl_test= DataLoader(BTCSeqDataset(X_test, y_test_hard, y_test_soft, y_test_reg, t_test_indices), batch_size=BATCH)\n",
    "else:\n",
    "    dl_test = None\n",
    "    print(\"Test DataLoader (dl_test) is not created due to empty X_test.\")\n",
    "    \n",
    "# Step 9: PositionalEncoding + TransformerClassifier with SoftLabel Loss\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000)/d_model))\n",
    "        pe[:,0::2] = torch.sin(pos * div)\n",
    "        pe[:,1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))\n",
    "    def forward(self, x): return x + self.pe[:x.size(0)]\n",
    "\n",
    "class TransformerClassifierSoftLabel(nn.Module):\n",
    "    def __init__(self, d_model=120, nhead=3, num_layers=2, num_classes=3, lambda_reg=0.1):\n",
    "        super().__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.proj = nn.Linear(4, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        layer = nn.TransformerEncoderLayer(d_model, nhead=nhead, dim_feedforward=256)\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))\n",
    "        self.regressor  = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x).permute(1,0,2)\n",
    "        h = self.pos_enc(h)\n",
    "        h = self.encoder(h)\n",
    "        cls = h[-1]\n",
    "        return self.classifier(cls), self.regressor(cls).squeeze(-1)\n",
    "\n",
    "    def compute_loss(self, logits, y_hard, y_soft, reg, y_reg):\n",
    "        # 分類損失: KLDiv between softmax(logits) & soft labels\n",
    "        loss_cls = F.kl_div(F.log_softmax(logits, dim=-1), y_soft, reduction='batchmean')\n",
    "        # 回帰損失: MSE\n",
    "        loss_reg = F.mse_loss(reg, y_reg)\n",
    "        loss = loss_cls + self.lambda_reg * loss_reg\n",
    "        return loss, loss_cls.item(), loss_reg.item()\n",
    "\n",
    "model = TransformerClassifierSoftLabel().to(device)\n",
    "opt = Adam(model.parameters(), lr=1e-4)\n",
    "sched = CosineAnnealingLR(opt, T_max=10)\n",
    "\n",
    "# 指標出力のためのヘルパー関数 (修正版)\n",
    "\n",
    "# --- 必要なライブラリ ---\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "\n",
    "# --- Helper Function: 閾値ベースの3クラス予測ラベル生成 (変更なし) ---\n",
    "def get_thresholded_predictions_final(probs_np, threshold_val):\n",
    "    y_pred_thresh = np.zeros(len(probs_np), dtype=int) \n",
    "    prob_min_scores = probs_np[:, 1] \n",
    "    prob_max_scores = probs_np[:, 2] \n",
    "\n",
    "    for i in range(len(probs_np)):\n",
    "        is_min_candidate = prob_min_scores[i] >= threshold_val\n",
    "        is_max_candidate = prob_max_scores[i] >= threshold_val\n",
    "\n",
    "        if is_min_candidate and is_max_candidate:\n",
    "            if prob_min_scores[i] >= prob_max_scores[i]:\n",
    "                y_pred_thresh[i] = 1 \n",
    "            else:\n",
    "                y_pred_thresh[i] = 2 \n",
    "        elif is_min_candidate:\n",
    "            y_pred_thresh[i] = 1 \n",
    "        elif is_max_candidate:\n",
    "            y_pred_thresh[i] = 2 \n",
    "    return y_pred_thresh\n",
    "\n",
    "# --- Helper Function: 将来変動率およびソフトラベル確率成分の統計計算 (修正) ---\n",
    "def calculate_signal_statistics(y_predictions_at_threshold, \n",
    "                                original_indices_np, \n",
    "                                source_df, # 'close', 'low', 'high', 'soft_label' カラムを含む想定\n",
    "                                class_index_to_analyze, # 分析対象の予測クラス (1 for Min, 2 for Max)\n",
    "                                num_future_steps):\n",
    "    \"\"\"\n",
    "    指定された予測クラスのサンプルについて、以下を計算して返します:\n",
    "    1. 将来の最安値への価格変動率(%)の統計 (mean, median, q1, q3)\n",
    "    2. 将来の最高値への価格変動率(%)の統計 (mean, median, q1, q3)\n",
    "    3. ソフトラベルのMin成分の統計 (mean, median, q1, q3, std)\n",
    "    4. ソフトラベルのMax成分の統計 (mean, median, q1, q3, std)\n",
    "    5. そのクラスの総予測数\n",
    "    \"\"\"\n",
    "    changes_to_low_list = []\n",
    "    changes_to_high_list = []\n",
    "    sl_min_component_scores = [] \n",
    "    sl_max_component_scores = [] \n",
    "    \n",
    "    predicted_as_class_indices = np.where(y_predictions_at_threshold == class_index_to_analyze)[0]\n",
    "    total_predictions_for_class = len(predicted_as_class_indices)\n",
    "\n",
    "    stats_template = {'mean': np.nan, 'median': np.nan, 'q1': np.nan, 'q3': np.nan}\n",
    "    sl_component_stats_template = {'mean': np.nan, 'median': np.nan, 'q1': np.nan, 'q3': np.nan, 'std': np.nan}\n",
    "\n",
    "    if total_predictions_for_class == 0:\n",
    "        return (stats_template, stats_template, \n",
    "                sl_component_stats_template, sl_component_stats_template, \n",
    "                total_predictions_for_class)\n",
    "\n",
    "    has_soft_label_col = 'soft_label' in source_df.columns\n",
    "    if not has_soft_label_col:\n",
    "        # この警告は呼び出し側(perform_detailed_evaluation)でまとめて出す方が重複を避けられる\n",
    "        # print(f\"WARNING: 'soft_label' column not found in source_df. SL component stats will be N/A.\")\n",
    "        pass # calculate_signal_statistics内では警告せず、データ欠損として扱う\n",
    "    \n",
    "    for arr_idx in predicted_as_class_indices:\n",
    "        original_df_idx = original_indices_np[arr_idx]\n",
    "        if not (0 <= original_df_idx < len(source_df)): \n",
    "            changes_to_low_list.append(np.nan); changes_to_high_list.append(np.nan)\n",
    "            sl_min_component_scores.append(np.nan); sl_max_component_scores.append(np.nan)\n",
    "            continue \n",
    "            \n",
    "        current_data_point = source_df.iloc[original_df_idx]\n",
    "        current_close = current_data_point.get('close', np.nan) # .getで存在しない場合もケア\n",
    "        \n",
    "        current_sl_min_comp = np.nan\n",
    "        current_sl_max_comp = np.nan\n",
    "        if has_soft_label_col:\n",
    "            soft_label_entry = current_data_point['soft_label']\n",
    "            if isinstance(soft_label_entry, (list, tuple)) and len(soft_label_entry) == 3:\n",
    "                try:\n",
    "                    current_sl_min_comp = float(soft_label_entry[1])\n",
    "                    current_sl_max_comp = float(soft_label_entry[2])\n",
    "                except (ValueError, TypeError, IndexError):\n",
    "                    # 要素が期待通りでない場合は NaN のまま\n",
    "                    pass \n",
    "        sl_min_component_scores.append(current_sl_min_comp)\n",
    "        sl_max_component_scores.append(current_sl_max_comp)\n",
    "\n",
    "        future_start_idx = original_df_idx + 1\n",
    "        future_end_idx = original_df_idx + 1 + num_future_steps\n",
    "        if future_end_idx <= len(source_df):\n",
    "            future_window_df = source_df.iloc[future_start_idx:future_end_idx]\n",
    "            if future_window_df.empty: \n",
    "                changes_to_low_list.append(np.nan); changes_to_high_list.append(np.nan)\n",
    "                continue\n",
    "            \n",
    "            future_low_price = future_window_df['low'].min() if not future_window_df['low'].empty else np.nan\n",
    "            future_high_price = future_window_df['high'].max() if not future_window_df['high'].empty else np.nan\n",
    "            \n",
    "            if pd.isna(future_low_price) or pd.isna(current_close) or current_close == 0: changes_to_low_list.append(np.nan)\n",
    "            else: changes_to_low_list.append(((future_low_price - current_close) / current_close) * 100)\n",
    "            \n",
    "            if pd.isna(future_high_price) or pd.isna(current_close) or current_close == 0: changes_to_high_list.append(np.nan)\n",
    "            else: changes_to_high_list.append(((future_high_price - current_close) / current_close) * 100)\n",
    "        else: \n",
    "            changes_to_low_list.append(np.nan); changes_to_high_list.append(np.nan)\n",
    "\n",
    "    stats_low = stats_template.copy()\n",
    "    changes_to_low_cleaned = [c for c in changes_to_low_list if not pd.isna(c)]\n",
    "    if changes_to_low_cleaned:\n",
    "        stats_low.update({'mean': np.mean(changes_to_low_cleaned), 'median': np.median(changes_to_low_cleaned),\n",
    "                          'q1': np.percentile(changes_to_low_cleaned, 25), 'q3': np.percentile(changes_to_low_cleaned, 75)})\n",
    "        \n",
    "    stats_high = stats_template.copy()\n",
    "    changes_to_high_cleaned = [c for c in changes_to_high_list if not pd.isna(c)]\n",
    "    if changes_to_high_cleaned: \n",
    "        stats_high.update({'mean': np.mean(changes_to_high_cleaned), 'median': np.median(changes_to_high_cleaned),\n",
    "                           'q1': np.percentile(changes_to_high_cleaned, 25), 'q3': np.percentile(changes_to_high_cleaned, 75)})\n",
    "\n",
    "    sl_min_comp_stats = sl_component_stats_template.copy()\n",
    "    sl_min_components_cleaned = [s for s in sl_min_component_scores if not pd.isna(s)]\n",
    "    if sl_min_components_cleaned:\n",
    "        sl_min_comp_stats.update({\n",
    "            'mean': np.mean(sl_min_components_cleaned), 'median': np.median(sl_min_components_cleaned),\n",
    "            'q1': np.percentile(sl_min_components_cleaned, 25), 'q3': np.percentile(sl_min_components_cleaned, 75),\n",
    "            'std': np.std(sl_min_components_cleaned)\n",
    "        })\n",
    "    \n",
    "    sl_max_comp_stats = sl_component_stats_template.copy()\n",
    "    sl_max_components_cleaned = [s for s in sl_max_component_scores if not pd.isna(s)]\n",
    "    if sl_max_components_cleaned:\n",
    "        sl_max_comp_stats.update({\n",
    "            'mean': np.mean(sl_max_components_cleaned), 'median': np.median(sl_max_components_cleaned),\n",
    "            'q1': np.percentile(sl_max_components_cleaned, 25), 'q3': np.percentile(sl_max_components_cleaned, 75),\n",
    "            'std': np.std(sl_max_components_cleaned)\n",
    "        })\n",
    "            \n",
    "    return stats_low, stats_high, sl_min_comp_stats, sl_max_comp_stats, total_predictions_for_class\n",
    "\n",
    "# --- Helper Function: 将来指標統計とSL成分統計をテーブル行としてフォーマット (修正) ---\n",
    "def format_future_stats_table_row(pred_type_str, \n",
    "                                  stats_low_dict, stats_high_dict, \n",
    "                                  sl_min_comp_stats_dict, sl_max_comp_stats_dict, \n",
    "                                  total_preds):\n",
    "    def f(val, precision=4): \n",
    "        return f\"{val:.{precision}f}\" if not pd.isna(val) else \"N/A\"\n",
    "    \n",
    "    # ヘッダーに合わせて列幅を調整 (perform_detailed_evaluation内のheader_cols定義と一致させる)\n",
    "    # \"Prediction Type\".ljust(19)\n",
    "    # \"Fut_Low_Mean\".rjust(12), \"Fut_Low_Median\".rjust(14), \"Fut_Low_Q1\".rjust(10), \"Fut_Low_Q3\".rjust(10)\n",
    "    # \"Fut_High_Mean\".rjust(13), \"Fut_High_Median\".rjust(15), \"Fut_High_Q1\".rjust(11), \"Fut_High_Q3\".rjust(11)\n",
    "    # \"SLMinC_Mean\".rjust(13), \"SLMinC_Median\".rjust(14), \"SLMinC_Q1\".rjust(10), \"SLMinC_Q3\".rjust(10), \"SLMinC_Std\".rjust(10)\n",
    "    # \"SLMaxC_Mean\".rjust(13), \"SLMaxC_Median\".rjust(14), \"SLMaxC_Q1\".rjust(10), \"SLMaxC_Q3\".rjust(10), \"SLMaxC_Std\".rjust(10)\n",
    "    # \"N_Pred\".rjust(7)\n",
    "    \n",
    "    row = f\"| {pred_type_str:<17} | {f(stats_low_dict['mean']):>12} | {f(stats_low_dict['median']):>14} | {f(stats_low_dict['q1']):>10} | {f(stats_low_dict['q3']):>10} | \"\n",
    "    row += f\"{f(stats_high_dict['mean']):>13} | {f(stats_high_dict['median']):>15} | {f(stats_high_dict['q1']):>11} | {f(stats_high_dict['q3']):>11} | \"\n",
    "    row += f\"{f(sl_min_comp_stats_dict['mean']):>13} | {f(sl_min_comp_stats_dict['median']):>14} | {f(sl_min_comp_stats_dict['q1']):>10} | {f(sl_min_comp_stats_dict['q3']):>10} | {f(sl_min_comp_stats_dict['std']):>10} | \"\n",
    "    row += f\"{f(sl_max_comp_stats_dict['mean']):>13} | {f(sl_max_comp_stats_dict['median']):>14} | {f(sl_max_comp_stats_dict['q1']):>10} | {f(sl_max_comp_stats_dict['q3']):>10} | {f(sl_max_comp_stats_dict['std']):>10} | \"\n",
    "    row += f\"{str(total_preds):>7} |\"\n",
    "    return row\n",
    "\n",
    "# --- Main Evaluation Function (修正版) ---\n",
    "def perform_detailed_evaluation(model_instance, dataloader_to_eval, original_df_for_lookup,\n",
    "                                device_to_use, output_path, file_prefix_str,\n",
    "                                class_target_names, prob_thresholds, num_future_steps,\n",
    "                                epoch_info_str=\"N/A\"): # s_min/max_col_nameはcalculate_signal_statistics内でデフォルト使用\n",
    "    print(f\"\\n--- Starting Detailed Evaluation for: {file_prefix_str} (Source Epoch for Model: {epoch_info_str}) ---\")\n",
    "    if dataloader_to_eval is None: print(f\"ERROR: DataLoader for '{file_prefix_str}' is None.\"); return\n",
    "\n",
    "    if 'soft_label' not in original_df_for_lookup.columns:\n",
    "        print(f\"WARNING: 'soft_label' column not found in 'original_df_for_lookup'. Statistics for soft label components will be N/A for all predictions.\")\n",
    "        # この場合でも処理は続行し、calculate_signal_statistics が NaN を適切に処理する\n",
    "\n",
    "    model_instance.eval() \n",
    "    all_true_labels, all_probs, all_original_indices = [], [], []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        eval_pbar = tqdm(dataloader_to_eval, desc=f\"Evaluating {file_prefix_str}\", leave=False)\n",
    "        for Xb, yh_hard, _, _, t_idx in eval_pbar: \n",
    "            Xb = Xb.to(device_to_use)\n",
    "            logits, _ = model_instance(Xb) \n",
    "            all_probs.append(torch.softmax(logits, dim=-1).cpu().numpy())\n",
    "            all_true_labels.append(yh_hard.cpu().numpy())\n",
    "            all_original_indices.append(t_idx.cpu().numpy())\n",
    "\n",
    "    if not all_true_labels: print(f\"ERROR: No data from DataLoader for '{file_prefix_str}'.\"); return\n",
    "    y_true_np = np.concatenate(all_true_labels)\n",
    "    probs_np = np.concatenate(all_probs)\n",
    "    original_indices_np = np.concatenate(all_original_indices)\n",
    "    if not (len(y_true_np) == len(probs_np) == len(original_indices_np)):\n",
    "        print(f\"ERROR: Mismatch in array lengths for {file_prefix_str}.\"); return\n",
    "\n",
    "    ep_info_fn = str(epoch_info_str).replace(\" \", \"_\").replace(\":\", \"-\")\n",
    "    metrics_filepath = os.path.join(output_path, f\"{file_prefix_str}_metrics_ep{ep_info_fn}.txt\")\n",
    "    returns_filepath = os.path.join(output_path, f\"{file_prefix_str}_future_returns_ep{ep_info_fn}.txt\")\n",
    "\n",
    "    overall_auc_scores = {} \n",
    "    for class_idx, class_name in enumerate(class_target_names):\n",
    "        if class_idx == 0: continue \n",
    "        overall_auc_scores[class_name] = {'roc_auc': \"N/A\", 'pr_auc': \"N/A\"}\n",
    "        if np.any(y_true_np == class_idx) and np.any(y_true_np != class_idx):\n",
    "            try:\n",
    "                roc = roc_auc_score((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                prec, rec, _ = precision_recall_curve((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                pr = auc(rec, prec)\n",
    "                overall_auc_scores[class_name]['roc_auc'] = f\"{roc:.4f}\"\n",
    "                overall_auc_scores[class_name]['pr_auc'] = f\"{pr:.4f}\"\n",
    "            except ValueError: pass\n",
    "\n",
    "    with open(metrics_filepath, \"w\") as f_metrics, open(returns_filepath, \"w\") as f_returns:\n",
    "        f_metrics.write(f\"Detailed Metrics for {file_prefix_str} (Model from Epoch: {epoch_info_str})\\n\")\n",
    "        f_returns.write(f\"Future Returns Analysis for {file_prefix_str} (Model from Epoch: {epoch_info_str})\\n\")\n",
    "\n",
    "        f_metrics.write(\"=\"*50 + \"\\nOverall Class-Specific AUCs (Threshold-Independent):\\n\")\n",
    "        for cn_auc in [class_target_names[1], class_target_names[2]]:\n",
    "             f_metrics.write(f\"  {cn_auc} - ROC-AUC: {overall_auc_scores[cn_auc]['roc_auc']}, PR-AUC: {overall_auc_scores[cn_auc]['pr_auc']}\\n\")\n",
    "        f_metrics.write(\"=\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        header_cols = [ # ヘッダーの列名を調整、SLMinC/SLMaxC に変更\n",
    "            \"Prediction Type\".ljust(19), \n",
    "            \"Fut_Low_Mean\".rjust(12), \"Fut_Low_Median\".rjust(14), \"Fut_Low_Q1\".rjust(10), \"Fut_Low_Q3\".rjust(10), \n",
    "            \"Fut_High_Mean\".rjust(13), \"Fut_High_Median\".rjust(15), \"Fut_High_Q1\".rjust(11), \"Fut_High_Q3\".rjust(11),\n",
    "            \"SLMinC_Mean\".rjust(13), \"SLMinC_Median\".rjust(14), \"SLMinC_Q1\".rjust(10), \"SLMinC_Q3\".rjust(10), \"SLMinC_Std\".rjust(10), \n",
    "            \"SLMaxC_Mean\".rjust(13), \"SLMaxC_Median\".rjust(14), \"SLMaxC_Q1\".rjust(10), \"SLMaxC_Q3\".rjust(10), \"SLMaxC_Std\".rjust(10), \n",
    "            \"N_Pred\".rjust(7) \n",
    "        ]\n",
    "        returns_table_header = \"|\"+ \"\".join([f\"{col}|\" for col in header_cols]) + \"\\n\"\n",
    "        returns_table_separator = \"|\" + \"\".join([\"-\"*len(col) + \"|\" for col in header_cols]) + \"\\n\"\n",
    "        \n",
    "        for th in prob_thresholds: \n",
    "            f_metrics.write(f\"--- Metrics for Threshold: {th:.2f} ---\\n\")\n",
    "            # (CM, CR, F1スコアの書き込み - 変更なし)\n",
    "            y_pred_at_threshold = get_thresholded_predictions_final(probs_np, th)\n",
    "            cm = confusion_matrix(y_true_np, y_pred_at_threshold, labels=[0,1,2])\n",
    "            f_metrics.write(\"Confusion Matrix:\\n\" + np.array2string(cm) + \"\\n\\n\")\n",
    "            try:\n",
    "                cr = classification_report(y_true_np, y_pred_at_threshold, target_names=class_target_names, labels=[0,1,2], zero_division=0, digits=4)\n",
    "                f_metrics.write(\"Classification Report:\\n\" + cr + \"\\n\\n\")\n",
    "            except Exception as e_cr:\n",
    "                f_metrics.write(f\"Could not generate CR for threshold {th:.2f}: {e_cr}\\n\\n\")\n",
    "            f1_min = f1_score((y_true_np == 1).astype(int), (probs_np[:, 1] > th).astype(int), zero_division=0)\n",
    "            f1_max = f1_score((y_true_np == 2).astype(int), (probs_np[:, 2] > th).astype(int), zero_division=0)\n",
    "            f_metrics.write(f\"Binary F1-score (Min vs Rest) @{th:.2f}: {f1_min:.4f}\\n\")\n",
    "            f_metrics.write(f\"Binary F1-score (Max vs Rest) @{th:.2f}: {f1_max:.4f}\\n\")\n",
    "            f_metrics.write(\"-\" * 40 + \"\\n\\n\") \n",
    "\n",
    "            # --- f_returns への書き込み ---\n",
    "            f_returns.write(f\"--- Returns Analysis for Threshold: {th:.2f} ---\\n\")\n",
    "            f_returns.write(returns_table_header)\n",
    "            f_returns.write(returns_table_separator)\n",
    "            \n",
    "            # Min予測の統計\n",
    "            stats_min_low, stats_min_high, sl_min_comp_stats_min, sl_max_comp_stats_min, total_min_preds = calculate_signal_statistics(\n",
    "                y_pred_at_threshold, original_indices_np, original_df_for_lookup,\n",
    "                1, num_future_steps # class_index 1 for Min\n",
    "            )\n",
    "            f_returns.write(format_future_stats_table_row(\n",
    "                f\"{class_target_names[1]} Predictions\", stats_min_low, stats_min_high, \n",
    "                sl_min_comp_stats_min, sl_max_comp_stats_min, # 両方のSL成分統計を渡す\n",
    "                total_min_preds) + \"\\n\")\n",
    "\n",
    "            # Max予測の統計\n",
    "            stats_max_low, stats_max_high, sl_min_comp_stats_max, sl_max_comp_stats_max, total_max_preds = calculate_signal_statistics(\n",
    "                y_pred_at_threshold, original_indices_np, original_df_for_lookup,\n",
    "                2, num_future_steps # class_index 2 for Max\n",
    "            )\n",
    "            f_returns.write(format_future_stats_table_row(\n",
    "                f\"{class_target_names[2]} Predictions\", stats_max_low, stats_max_high, \n",
    "                sl_min_comp_stats_max, sl_max_comp_stats_max, # 両方のSL成分統計を渡す\n",
    "                total_max_preds) + \"\\n\")\n",
    "            \n",
    "            # Up/Down カウントと比率を f_returns に書き込み (テーブルの後)\n",
    "            f_returns.write(f\"\\nPrice Change Direction after {num_future_steps} steps (for signals at threshold {th:.2f}):\\n\")\n",
    "            for class_idx_ud, class_name_ud in [(1, class_target_names[1]), (2, class_target_names[2])]:\n",
    "                up_count = 0; down_count = 0; neutral_count = 0; valid_comparison_count = 0\n",
    "                predicted_indices_for_class = np.where(y_pred_at_threshold == class_idx_ud)[0]\n",
    "\n",
    "                if len(predicted_indices_for_class) > 0:\n",
    "                    for arr_idx_ud in predicted_indices_for_class:\n",
    "                        original_df_idx_ud = original_indices_np[arr_idx_ud]\n",
    "                        if not (0 <= original_df_idx_ud < len(original_df_for_lookup) and \\\n",
    "                                original_df_idx_ud + num_future_steps < len(original_df_for_lookup)):\n",
    "                            continue\n",
    "                        current_close_ud = original_df_for_lookup['close'].iloc[original_df_idx_ud]\n",
    "                        future_close_ud = original_df_for_lookup['close'].iloc[original_df_idx_ud + num_future_steps]\n",
    "                        if pd.isna(current_close_ud) or pd.isna(future_close_ud): continue\n",
    "                        valid_comparison_count += 1\n",
    "                        if future_close_ud > current_close_ud: up_count += 1\n",
    "                        elif future_close_ud < current_close_ud: down_count += 1\n",
    "                        else: neutral_count += 1\n",
    "                \n",
    "                up_down_total_for_ratio = up_count + down_count\n",
    "                up_pct_of_directional = f\"{(up_count / up_down_total_for_ratio * 100):.2f}%\" if up_down_total_for_ratio > 0 else \"N/A\"\n",
    "                direct_up_down_ratio = \"N/A\"\n",
    "                if down_count > 0: direct_up_down_ratio = f\"{(up_count / down_count):.2f}\"\n",
    "                elif up_count > 0: direct_up_down_ratio = \"Inf (all up)\" # No downs, only ups\n",
    "                \n",
    "                f_returns.write(f\"  For {class_name_ud} Pred (N_pred={len(predicted_indices_for_class)}, N_valid_comp={valid_comparison_count}): \"\n",
    "                                f\"Up: {up_count}, Down: {down_count}, Neutral: {neutral_count} | \"\n",
    "                                f\"Up Pct (of Up+Down): {up_pct_of_directional}, Up/Down Ratio: {direct_up_down_ratio}\\n\")\n",
    "            f_returns.write(\"-\" * 40 + \"\\n\\n\") # Threshold block in f_returns ends\n",
    "    \n",
    "    print(f\"INFO: Detailed metrics for '{file_prefix_str}' saved to: {metrics_filepath}\")\n",
    "    print(f\"INFO: Future returns analysis for '{file_prefix_str}' saved to: {returns_filepath}\")\n",
    "    print(f\"--- Finished Detailed Evaluation for: {file_prefix_str} ---\")\n",
    "    \n",
    "# Step 10: 学習ループ (tqdm + ライブプロット + 詳細評価 + ベストモデル保存)\n",
    "\n",
    "# --- このStep10のコードブロックを実行するための前提条件 ---\n",
    "# 以下のオブジェクト・変数は、これより前のStep (1-9) で定義・初期化されている必要があります:\n",
    "#   model, opt, sched, dl_tr, dl_val, df_valid, device, output_dir\n",
    "#   TransformerClassifierSoftLabel クラス定義\n",
    "#   ヘルパー関数: get_thresholded_predictions_final, log_future_price_change_stats_revised\n",
    "# 必要なライブラリ: torch, numpy as np, os, sklearn.metrics 各種, \n",
    "#                 IPython.display (display, clear_output), matplotlib.pyplot as plt, tqdm.auto\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(f\"INFO: Step 10 - Initializing variables and starting training loop.\")\n",
    "\n",
    "# --- Step10で必要となるパラメータと状態変数の定義・初期化 ---\n",
    "EPOCHS = 30  # 例: 総エポック数 (実際の値に合わせてください)\n",
    "thresholds = [0.25, 0.5, 0.75] # 評価に使用する確率閾値のリスト\n",
    "target_names = ['Other', 'Min', 'Max'] # クラスラベル名\n",
    "FUTURE_STEPS = 6 # 将来の価格変動を何ステップ先まで見るか\n",
    "\n",
    "# 学習履歴保存用\n",
    "history = {'train_total':[], 'train_cls':[], 'train_reg':[],\n",
    "           'val_total':[],   'val_cls':[],   'val_reg':[]}\n",
    "\n",
    "# ベストモデル保存用\n",
    "min_val_loss = float('inf')\n",
    "# output_dir は Step7 で定義されている想定\n",
    "best_model_path = os.path.join(output_dir, \"best_transformer_model.pth\") \n",
    "best_epoch = -1\n",
    "\n",
    "# ライブプロット用の図と軸を準備 (再実行時のために存在確認と初期化)\n",
    "# この fig_loss, ax_loss はこのStep10のブロック内で閉じることを推奨 (plt.close(fig_loss))\n",
    "# または、上位のスコープで管理し、ここではその存在を前提とする。\n",
    "# 今回は、このStep10内で完結するように初期化。\n",
    "if 'fig_loss' not in locals() or fig_loss is None or not plt.fignum_exists(fig_loss.number):\n",
    "    fig_loss, ax_loss = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    print(\"INFO: Step 10 - Loss plot figure initialized.\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "print(f\"INFO: Starting training and evaluation loop for {EPOCHS} epochs.\")\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    # --- Train Phase ---\n",
    "    model.train()\n",
    "    train_run_metrics = {'total_loss': 0.0, 'cls_loss': 0.0, 'reg_loss': 0.0}\n",
    "    num_train_batches = 0\n",
    "    train_pbar = tqdm(dl_tr, desc=f\"Epoch {ep}/{EPOCHS} [Train]\", leave=False)\n",
    "    for Xb, yh_hard, ys_soft, y_reg_target, _ in train_pbar: # t_indices は訓練では未使用\n",
    "        Xb, yh_hard, ys_soft, y_reg_target = Xb.to(device), yh_hard.to(device), ys_soft.to(device), y_reg_target.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        logits, reg_pred = model(Xb)\n",
    "        loss, lcls, lreg = model.compute_loss(logits, yh_hard, ys_soft, reg_pred, y_reg_target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_run_metrics['total_loss'] += loss.item()\n",
    "        train_run_metrics['cls_loss'] += lcls # .item() は compute_loss 内で行われている前提\n",
    "        train_run_metrics['reg_loss'] += lreg # .item() は compute_loss 内で行われている前提\n",
    "        num_train_batches += 1\n",
    "        train_pbar.set_postfix({k_train: v_train / num_train_batches for k_train, v_train in train_run_metrics.items()})\n",
    "\n",
    "    if hasattr(sched, 'step'): # スケジューラが存在し、stepメソッドを持つ場合\n",
    "        sched.step()\n",
    "    \n",
    "    history['train_total'].append(train_run_metrics['total_loss'] / num_train_batches if num_train_batches > 0 else 0)\n",
    "    history['train_cls'].append(train_run_metrics['cls_loss'] / num_train_batches if num_train_batches > 0 else 0)\n",
    "    history['train_reg'].append(train_run_metrics['reg_loss'] / num_train_batches if num_train_batches > 0 else 0)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    if dl_val: # dl_val が None でないことを確認\n",
    "        model.eval()\n",
    "        val_run_metrics = {'total_loss': 0.0, 'cls_loss': 0.0, 'reg_loss': 0.0}\n",
    "        num_val_batches = 0\n",
    "        y_true_val_hard_all, probs_val_all, t_indices_val_all = [], [], []\n",
    "        \n",
    "        val_pbar = tqdm(dl_val, desc=f\"Epoch {ep}/{EPOCHS} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for Xb_val, yh_hard_val, ys_soft_val, y_reg_target_val, t_idx_val in val_pbar:\n",
    "                Xb_val, yh_hard_val, ys_soft_val, y_reg_target_val = Xb_val.to(device), yh_hard_val.to(device), ys_soft_val.to(device), y_reg_target_val.to(device)\n",
    "                \n",
    "                logits_val, reg_pred_val = model(Xb_val)\n",
    "                loss_val, lcls_val, lreg_val = model.compute_loss(logits_val, yh_hard_val, ys_soft_val, reg_pred_val, y_reg_target_val)\n",
    "                \n",
    "                val_run_metrics['total_loss'] += loss_val.item()\n",
    "                val_run_metrics['cls_loss'] += lcls_val\n",
    "                val_run_metrics['reg_loss'] += lreg_val\n",
    "                num_val_batches += 1\n",
    "                val_pbar.set_postfix({k_val: v_val / num_val_batches for k_val, v_val in val_run_metrics.items()})\n",
    "\n",
    "                probs_val_all.append(torch.softmax(logits_val, dim=-1).cpu().numpy())\n",
    "                y_true_val_hard_all.append(yh_hard_val.cpu().numpy())\n",
    "                t_indices_val_all.append(t_idx_val.cpu().numpy())\n",
    "\n",
    "        current_epoch_val_total_loss = val_run_metrics['total_loss'] / num_val_batches if num_val_batches > 0 else float('inf')\n",
    "        history['val_total'].append(current_epoch_val_total_loss)\n",
    "        history['val_cls'].append(val_run_metrics['cls_loss'] / num_val_batches if num_val_batches > 0 else 0)\n",
    "        history['val_reg'].append(val_run_metrics['reg_loss'] / num_val_batches if num_val_batches > 0 else 0)\n",
    "\n",
    "        if current_epoch_val_total_loss < min_val_loss:\n",
    "            min_val_loss = current_epoch_val_total_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            best_epoch = ep\n",
    "            # tqdmを使っている場合、printがバーを乱すことがあるため、ループ外での最終報告を推奨\n",
    "            # print(f\"\\nEpoch {ep}: New best model saved! Val Total Loss: {min_val_loss:.4f}\") \n",
    "        \n",
    "        # --- Validation Metrics & Future Returns Analysis ---\n",
    "        if num_val_batches > 0:\n",
    "            y_true_np = np.concatenate(y_true_val_hard_all)\n",
    "            probs_np = np.concatenate(probs_val_all)\n",
    "            val_original_indices_np = np.concatenate(t_indices_val_all)\n",
    "\n",
    "            # ファイル名はエポック番号を3桁ゼロ埋めにするなど工夫するとソートしやすい\n",
    "            metrics_filepath = os.path.join(output_dir, f\"validation_metrics_ep{ep:03d}.txt\")\n",
    "            returns_filepath = os.path.join(output_dir, f\"validation_future_returns_ep{ep:03d}.txt\")\n",
    "\n",
    "            with open(metrics_filepath, \"w\") as f_metrics, open(returns_filepath, \"w\") as f_returns:\n",
    "                f_metrics.write(f\"Epoch {ep} - Validation Set Metrics\\n\")\n",
    "                f_returns.write(f\"Epoch {ep} - Validation Set Future Returns Analysis\\n\")\n",
    "                \n",
    "                f_metrics.write(\"=\"*50 + \"\\nOverall Class-Specific AUCs (Threshold-Independent):\\n\")\n",
    "                for class_idx, class_name in enumerate(target_names):\n",
    "                    if class_idx == 0: continue # Skip 'Other' class for this specific AUC reporting\n",
    "                    if np.any(y_true_np == class_idx) and np.any(y_true_np != class_idx):\n",
    "                        try:\n",
    "                            roc_auc = roc_auc_score((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                            precision, recall, _ = precision_recall_curve((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                            pr_auc = auc(recall, precision)\n",
    "                            f_metrics.write(f\"  {class_name} - ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\\n\")\n",
    "                        except ValueError as e_auc: # ハンドリングを追加\n",
    "                             f_metrics.write(f\"  {class_name} - ROC-AUC: Error ({e_auc}), PR-AUC: Error\\n\")\n",
    "                    else:\n",
    "                        f_metrics.write(f\"  {class_name} - ROC-AUC: N/A, PR-AUC: N/A (Insufficient class diversity or no samples for this class)\\n\")\n",
    "                f_metrics.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "                for th in thresholds:\n",
    "                    f_metrics.write(f\"--- Metrics for Threshold: {th:.2f} ---\\n\")\n",
    "                    f_returns.write(f\"--- Returns Analysis for Threshold: {th:.2f} ---\\n\")\n",
    "\n",
    "                    # get_thresholded_predictions_final は事前に定義されている前提\n",
    "                    y_pred_at_threshold = get_thresholded_predictions_final(probs_np, th)\n",
    "                    \n",
    "                    cm = confusion_matrix(y_true_np, y_pred_at_threshold, labels=[0,1,2])\n",
    "                    f_metrics.write(\"Confusion Matrix:\\n\" + np.array2string(cm) + \"\\n\\n\")\n",
    "                    try:\n",
    "                        cr = classification_report(y_true_np, y_pred_at_threshold, target_names=target_names, labels=[0,1,2], zero_division=0, digits=4)\n",
    "                        f_metrics.write(\"Classification Report:\\n\" + cr + \"\\n\\n\")\n",
    "                    except Exception as e_cr: \n",
    "                        f_metrics.write(f\"Could not generate Classification Report for threshold {th:.2f}: {e_cr}\\n\\n\")\n",
    "\n",
    "                    f1_min = f1_score((y_true_np == 1).astype(int), (probs_np[:, 1] > th).astype(int), zero_division=0)\n",
    "                    f1_max = f1_score((y_true_np == 2).astype(int), (probs_np[:, 2] > th).astype(int), zero_division=0)\n",
    "                    f_metrics.write(f\"Binary F1-score (Min vs Rest) @{th:.2f}: {f1_min:.4f}\\n\")\n",
    "                    f_metrics.write(f\"Binary F1-score (Max vs Rest) @{th:.2f}: {f1_max:.4f}\\n\")\n",
    "                    f_metrics.write(\"-\" * 40 + \"\\n\\n\")\n",
    "                    \n",
    "                    # log_future_price_change_stats_revised は事前に定義されている前提\n",
    "                    # df_valid は上位スコープから参照\n",
    "                    log_future_price_change_stats_revised(f_returns, y_pred_at_threshold, val_original_indices_np, df_valid,\n",
    "                                                          1, target_names[1], FUTURE_STEPS, th)\n",
    "                    log_future_price_change_stats_revised(f_returns, y_pred_at_threshold, val_original_indices_np, df_valid,\n",
    "                                                          2, target_names[2], FUTURE_STEPS, th)\n",
    "            \n",
    "            # tqdmを使っている場合、printがバーを乱すことがあるため、ループ外での最終報告を推奨\n",
    "            # if ep < EPOCHS: print() # ループの最後以外で改行を入れるなど工夫\n",
    "    else: # dl_val がない場合\n",
    "        history['val_total'].append(float('inf')) # val_lossは無限大として扱う\n",
    "        history['val_cls'].append(0)\n",
    "        history['val_reg'].append(0)\n",
    "\n",
    "    # --- Plot Losses ---\n",
    "    clear_output(wait=True) # Jupyter Notebook/Lab環境でプロットを更新表示\n",
    "    for i, key_suffix in enumerate(['total', 'cls', 'reg']):\n",
    "        ax = ax_loss[i] # ax_loss[i] を ax に代入\n",
    "        ax.clear()\n",
    "        ax.plot(history[f'train_{key_suffix}'], label=f'train_{key_suffix}')\n",
    "        # valのロスが記録されているか、かつinfでない場合のみプロット\n",
    "        if dl_val and len(history[f'val_{key_suffix}']) > 0 and not np.all(np.isinf(history[f'val_{key_suffix}'])):\n",
    "             ax.plot(history[f'val_{key_suffix}'], label=f'val_{key_suffix}')\n",
    "        ax.set_title(f\"{key_suffix.capitalize()} Loss (Epoch {ep})\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Average Loss\")\n",
    "        ax.legend()\n",
    "    fig_loss.tight_layout()\n",
    "    display(fig_loss) # Jupyter Notebook/Lab環境で図を表示\n",
    "\n",
    "# --- End of Training Loop ---\n",
    "print(\"\\nINFO: Step 10 - Training Loop Finished ---\")\n",
    "if best_epoch != -1:\n",
    "    print(f\"Best model found at epoch {best_epoch} with validation total loss: {min_val_loss:.4f}\")\n",
    "    print(f\"Best model weights saved to: {best_model_path}\")\n",
    "else:\n",
    "    print(\"No best model was saved (either validation loss did not improve or no validation data was provided).\")\n",
    "\n",
    "# 学習ループ終了後にプロットウィンドウを閉じる場合は以下を有効化\n",
    "# plt.close(fig_loss)\n",
    "\n",
    "# Step 11: テスト評価 (詳細評価) - Step10からの独立性を確保したバージョン\n",
    "\n",
    "# --- 必要なライブラリ (スクリプトの先頭やヘルパー関数定義部でインポート済みと仮定) ---\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from tqdm.auto import tqdm # perform_detailed_evaluation内で使用\n",
    "# (perform_detailed_evaluation およびそのヘルパー関数内で必要な sklearn.metrics なども同様)\n",
    "\n",
    "# --- このStep11のコードブロックを実行するための前提条件 ---\n",
    "# 1. モデルクラス定義 (事前にグローバルスコープで定義されていること):\n",
    "#    TransformerClassifierSoftLabel \n",
    "# 2. ヘルパー関数群 (事前にグローバルスコープで定義されていること):\n",
    "#    get_thresholded_predictions_final(probs_np, threshold_val)\n",
    "#    calculate_future_price_stats(y_predictions_at_threshold, ...) # 最新版のシグネチャに注意\n",
    "#    format_future_stats_table_row(pred_type_str, ...)          # 最新版のシグネチャに注意\n",
    "#    perform_detailed_evaluation(model_instance, ...) \n",
    "#      (これらの関数の定義は、以前の回答で提示した「指標出力のためのヘルパー関数 (修正版)」を参照)\n",
    "# 3. 必須オブジェクト・変数 (以前のステップで準備され、グローバルスコープで利用可能であること):\n",
    "#   - dl_test: torch.utils.data.DataLoader - テスト用データローダー (Step8で作成)\n",
    "#   - df_valid: pd.DataFrame - テストデータのインデックスがマッピングできる元のDataFrame (Step5でロード・処理)\n",
    "#                             (★注意: テストデータがdf_valid由来でない場合、適切なDataFrameを指定してください)\n",
    "#   - device: str - 計算デバイス (Step3で定義)\n",
    "#   - output_dir: str - 結果ファイル等を保存するディレクトリパス (Step7で定義)\n",
    "#   - SEQ_LEN: int - モデル入力のシーケンス長 (Step7で定義)\n",
    "#\n",
    "#   注意: 以下の評価パラメータ (TARGET_NAMES_S11, THRESHOLDS_S11, FUTURE_STEPS_S11) は\n",
    "#         このStep11ブロック内で定義されます。もしStep10の検証評価と設定を共通化したい場合は、\n",
    "#         これらのパラメータをスクリプトのより上位（共通設定箇所）で定義し、\n",
    "#         Step10とStep11の両方がそれを参照するように変更することを強く推奨します。\n",
    "#         これにより、設定の不整合を防ぎ、管理が容易になります。\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nINFO: Step 11 - Starting Test Set Evaluation.\")\n",
    "print(\"INFO: This step defines its own evaluation parameters for independence.\")\n",
    "print(\"      It relies on a saved model specified by 'BEST_MODEL_PATH_S11'.\")\n",
    "\n",
    "# --- Step 11 固有の評価パラメータと設定をここで定義 ---\n",
    "TARGET_NAMES_S11 = ['Other', 'Min', 'Max']    # クラスラベル名\n",
    "THRESHOLDS_S11 = [0.25, 0.5, 0.75]        # 評価に使用する確率閾値のリスト\n",
    "FUTURE_STEPS_S11 = 6                     # 将来の価格変動を何ステップ先まで見るか\n",
    "\n",
    "# ベストモデルのパスは output_dir (Step7で定義済み想定) と固定ファイル名で構築\n",
    "# このパスに Step10 などで学習・保存されたモデルが存在することを期待します。\n",
    "BEST_MODEL_PATH_S11 = os.path.join(output_dir, \"best_transformer_model.pth\") \n",
    "\n",
    "# Step10の実行結果である 'best_epoch' には依存しないため、\n",
    "# perform_detailed_evaluation に渡すエポック情報は固定の文字列、またはパスから類推できる情報とします。\n",
    "EPOCH_INFO_FOR_FILE_S11 = \"best_model_file\" # 例: \"best_model_from_file\" や \"loaded_best_model\" など\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# --- 必須グローバル変数の存在チェック (主要なもの) ---\n",
    "essential_globals_s11 = {\n",
    "    'dl_test': \"Test DataLoader\", 'df_valid': \"Source DataFrame for test context\", \n",
    "    'device': \"Device string\", 'output_dir': \"Output directory path\",\n",
    "    'SEQ_LEN': \"Sequence length parameter\", \n",
    "    'TransformerClassifierSoftLabel': \"Model class definition\",\n",
    "    'perform_detailed_evaluation': \"Main evaluation helper function\",\n",
    "    'get_thresholded_predictions_final': \"Prediction helper function\",\n",
    "    'calculate_future_price_stats': \"Statistics helper function\", # 最新の関数名に注意\n",
    "    'format_future_stats_table_row': \"Table formatting helper function\" # 最新の関数名に注意\n",
    "}\n",
    "all_prerequisites_available_s11 = True\n",
    "for var, name in essential_globals_s11.items():\n",
    "    if var not in globals() and var not in locals(): \n",
    "        print(f\"ERROR: Prerequisite '{name}' (variable/function: {var}) for Step 11 is not defined.\")\n",
    "        all_prerequisites_available_s11 = False\n",
    "\n",
    "if not all_prerequisites_available_s11:\n",
    "    raise NameError(\"Missing one or more crucial prerequisites for Step 11. Evaluation cannot proceed.\")\n",
    "\n",
    "if not os.path.exists(output_dir): \n",
    "    os.makedirs(output_dir)\n",
    "    print(f\"INFO: Created output directory for Step 11 results: {output_dir}\")\n",
    "# ---------------------------------------------\n",
    "\n",
    "# テスト用の新しいモデルインスタンスを生成\n",
    "model_for_testing_s11 = TransformerClassifierSoftLabel().to(device) # 学習時と同じパラメータで初期化\n",
    "evaluation_performed_s11 = False\n",
    "\n",
    "if dl_test is not None:\n",
    "    if os.path.exists(BEST_MODEL_PATH_S11):\n",
    "        print(f\"INFO: Found model file at '{BEST_MODEL_PATH_S11}'. Attempting to load weights...\")\n",
    "        try:\n",
    "            model_for_testing_s11.load_state_dict(torch.load(BEST_MODEL_PATH_S11, map_location=device))\n",
    "            print(f\"INFO: Successfully loaded model weights from '{BEST_MODEL_PATH_S11}'.\")\n",
    "            \n",
    "            # perform_detailed_evaluation 関数を呼び出し\n",
    "            # この関数は、必要なソフトラベルスコアカラム名 (s_min_raw_score_col_nameなど) を\n",
    "            # original_df_for_lookup (ここでは df_valid) が含んでいることを期待します。\n",
    "            # 必要に応じて、perform_detailed_evaluationの呼び出し時にカラム名を指定してください。\n",
    "            perform_detailed_evaluation(\n",
    "                model_instance=model_for_testing_s11,\n",
    "                dataloader_to_eval=dl_test,\n",
    "                original_df_for_lookup=df_valid, # ★重要: テストデータがdf_valid由来でない場合は、ここで適切なDataFrameを指定\n",
    "                device_to_use=device,\n",
    "                output_path=output_dir,\n",
    "                file_prefix_str=\"test_independent_eval\", # ファイル名接頭辞\n",
    "                class_target_names=TARGET_NAMES_S11,    # Step11ローカル定義を使用\n",
    "                prob_thresholds=THRESHOLDS_S11,       # Step11ローカル定義を使用\n",
    "                num_future_steps=FUTURE_STEPS_S11,    # Step11ローカル定義を使用\n",
    "                epoch_info_str=EPOCH_INFO_FOR_FILE_S11  # Step11ローカル定義の固定情報\n",
    "                # s_min_raw_score_col_name='s_min_raw_score', # 必要なら指定\n",
    "                # s_max_raw_score_col_name='s_max_raw_score'  # 必要なら指定\n",
    "            )\n",
    "            evaluation_performed_s11 = True\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load or evaluate model from '{BEST_MODEL_PATH_S11}'. Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(f\"ERROR: Specified model path '{BEST_MODEL_PATH_S11}' does not exist.\")\n",
    "        print(\"INFO: Cannot perform evaluation. Please ensure a trained model is saved at this path,\")\n",
    "        print(\"      or update 'BEST_MODEL_PATH_S11' (defined at the start of Step 11) to the correct file path.\")\n",
    "    \n",
    "    if not evaluation_performed_s11:\n",
    "        print(\"INFO: Step 11 - No evaluation was successfully performed with a trained model.\")\n",
    "else:\n",
    "    print(\"INFO: Test data loader (dl_test) is None. Skipping test evaluation.\")\n",
    "\n",
    "print(\"INFO: Step 11 - Test Evaluation Phase Finished.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
