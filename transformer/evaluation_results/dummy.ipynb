{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 環境セットアップ（Google Driveマウント＋ライブラリインポート）\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "except ImportError:\n",
    "    print(\"Drive mount skipped\")\n",
    "import math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Step 2: ファイルパス定義\n",
    "# TRAIN_PATH = \"/content/drive/MyDrive/TSAT/BTC_5min/BTC_full_5min_Train.csv\"\n",
    "# VALID_PATH = \"/content/drive/MyDrive/TSAT/BTC_5min/BTC_full_5min_Valid.csv\"\n",
    "\n",
    "TRAIN_PATH = \"/home/nagumo/TSAT/BTC_5min/BTC_full_5min_Train.csv\"\n",
    "VALID_PATH = \"/home/nagumo/TSAT/BTC_5min/BTC_full_5min_Valid.csv\"\n",
    "\n",
    "# Step 3: 再現性確保のためのシード固定\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Step 4: データ読み込み＆補間関数定義\n",
    "def load_and_interpolate(path):\n",
    "    df = pd.read_csv(path, parse_dates=['date']).set_index('date').sort_index()\n",
    "    idx = pd.date_range(df.index.min(), df.index.max(), freq='5T')\n",
    "    return df.reindex(idx).interpolate(method='time')\n",
    "\n",
    "# Step 5: データ読み込み実行＋期間フィルタリング\n",
    "df_train = load_and_interpolate(TRAIN_PATH)\n",
    "df_valid = load_and_interpolate(VALID_PATH)\n",
    "df_train = df_train[df_train.index >= '2020-01-01']\n",
    "print(\"Train:\", df_train.index.min(), \"to\", df_train.index.max())\n",
    "print(\"Valid:\", df_valid.index.min(), \"to\", df_valid.index.max())\n",
    "\n",
    "# Step 5: ハードラベル生成 (前後 k=6)\n",
    "def make_hard_labels(lows, highs, k=6):\n",
    "    n = len(lows)\n",
    "    lbl = np.zeros(n, dtype=int)\n",
    "    for t in range(k, n-k):\n",
    "        if lows[t] < lows[t-k:t].min() and lows[t] < lows[t+1:t+k+1].min():\n",
    "            lbl[t] = 1\n",
    "        elif highs[t] > highs[t-k:t].max() and highs[t] > highs[t+1:t+k+1].max():\n",
    "            lbl[t] = 2\n",
    "    return lbl\n",
    "\n",
    "lows_train, highs_train = df_train['low'].values, df_train['high'].values\n",
    "lows_val, highs_val     = df_valid['low'].values, df_valid['high'].values\n",
    "hard_train = make_hard_labels(lows_train, highs_train, k=6)\n",
    "hard_val   = make_hard_labels(lows_val, highs_val, k=6)\n",
    "df_train['hard_label'] = hard_train\n",
    "df_valid['hard_label'] = hard_val\n",
    "\n",
    "# Step 6: スコアベースソフトラベリング関数\n",
    "def compute_scores(lows, highs, k=6):\n",
    "    n = len(lows)\n",
    "    s_min = np.zeros(n)\n",
    "    s_max = np.zeros(n)\n",
    "    for t in range(k, n-k):\n",
    "        # 極小度\n",
    "        s_min[t] = 100*np.mean((lows[t-k:t] - lows[t]) / lows[t-k:t] +\n",
    "                           (lows[t+1:t+k+1] - lows[t]) / lows[t+1:t+k+1])\n",
    "        # 極大度\n",
    "        s_max[t] = 100*np.mean((highs[t] - highs[t-k:t]) / highs[t-k:t] +\n",
    "                           (highs[t] - highs[t+1:t+k+1]) / highs[t+1:t+k+1])\n",
    "    return s_min, s_max\n",
    "\n",
    "smin_train, smax_train = compute_scores(lows_train, highs_train, k=6)\n",
    "smin_val, smax_val     = compute_scores(lows_val, highs_val, k=6)\n",
    "\n",
    "def soft_label_from_scores(smin, smax):\n",
    "    # baseline for Other = 1.0\n",
    "    # logits = np.stack([np.ones_like(smin), smin, smax], axis=1)\n",
    "    logits = np.stack([1.5-np.maximum(smin, smax), smin, smax], axis=1) # other をsmin-smax\n",
    "    exp = np.exp(logits)\n",
    "    return exp / exp.sum(axis=1, keepdims=True)\n",
    "\n",
    "soft_train = soft_label_from_scores(smin_train, smax_train)\n",
    "soft_val   = soft_label_from_scores(smin_val, smax_val)\n",
    "df_train['soft_label'] = list(soft_train)\n",
    "df_valid['soft_label'] = list(soft_val)\n",
    "\n",
    "# ソフトラベリングの結果を確認\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- ソフトラベルを numpy array に展開 ---\n",
    "soft_array = np.array(df_train['soft_label'].tolist())\n",
    "soft_other = soft_array[:, 0]\n",
    "soft_min   = soft_array[:, 1]\n",
    "soft_max   = soft_array[:, 2]\n",
    "\n",
    "# --- タイムインデックスと価格 ---\n",
    "times       = df_train.index\n",
    "low_series  = df_train['low']\n",
    "high_series = df_train['high']\n",
    "close_series = df_train['close']\n",
    "\n",
    "# --- プロット範囲を 2021-01-01 ～ 2021-01-31 に限定 ---\n",
    "start, end = pd.to_datetime(\"2021-11-01-12:00:00\"), pd.to_datetime(\"2021-11-01-23:00:00\")\n",
    "mask = (times >= start) & (times <= end)\n",
    "\n",
    "# --- 抽出 ---\n",
    "times_f        = times[mask]\n",
    "low_f          = low_series[mask]\n",
    "high_f         = high_series[mask]\n",
    "close_f       = close_series[mask]\n",
    "soft_min_f     = soft_min[mask]\n",
    "soft_max_f     = soft_max[mask]\n",
    "soft_other_f   = soft_other[mask]\n",
    "\n",
    "# --- プロット1: Low Price vs Soft Min Score ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "ax1.plot(times_f, low_f, label='Low Price', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(times_f, soft_min_f, label='Soft Min Score', color='tab:orange', alpha=0.7)\n",
    "ax2.set_ylim(0,1)\n",
    "ax1.set_title('2021-01 Low Price vs Soft Min Score')\n",
    "ax1.set_ylabel('Low Price')\n",
    "ax2.set_ylabel('Soft Min Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- プロット2: High Price vs Soft Max Score ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "ax1.plot(times_f, high_f, label='High Price', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(times_f, soft_max_f, label='Soft Max Score', color='tab:green', alpha=0.7)\n",
    "ax2.set_ylim(0,1)\n",
    "ax1.set_title('2021-01 High Price vs Soft Max Score')\n",
    "ax1.set_ylabel('High Price')\n",
    "ax2.set_ylabel('Soft Max Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- プロット3: Close vs Soft Other Score ---\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "ax1.plot(times_f, close_f, label='Close Price', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(times_f, soft_min_f, label='Soft Min Score', color='tab:orange', alpha=0.7)\n",
    "ax2.plot(times_f, soft_max_f,label='Soft Max Score', color='tab:green', alpha=0.7)\n",
    "ax2.plot(times_f, soft_other_f, label='Soft Other Score', color='tab:red', alpha=0.7)\n",
    "ax2.set_ylim(0,1)\n",
    "ax1.set_title('2021-01 Close Price vs Soft Other Score')\n",
    "ax1.set_ylabel('Close Price')\n",
    "ax2.set_ylabel('Soft Other Score')\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 7: シーケンス特徴量 & ラベル生成 (SEQ_LEN=288)\n",
    "SEQ_LEN = 288\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm # tqdmライブラリをインポート (通常はスクリプトの先頭(Step1等)で一度だけインポートします)\n",
    "\n",
    "output_dir = \"evaluation_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def make_dataset(df, hard_lbl, soft_lbl, description=\"Creating dataset\"):\n",
    "    \"\"\"\n",
    "    DataFrameからシーケンス特徴量とラベルを生成します。\n",
    "    処理の進捗はtqdmプログレスバーで表示されます。\n",
    "a\n",
    "    Args:\n",
    "        df (pd.DataFrame): 入力DataFrame (価格データなどを含む)\n",
    "        hard_lbl (np.array): ハードラベル配列\n",
    "        soft_lbl (np.array): ソフトラベル配列\n",
    "        description (str, optional): tqdmプログレスバーに表示する説明文. Defaults to \"Creating dataset\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: X, y_hard, y_soft, y_reg, t_indices のnumpy配列を含むタプル\n",
    "    \"\"\"\n",
    "    X, y_hard, y_soft, y_reg, t_indices = [], [], [], [], []\n",
    "    \n",
    "    # DataFrameの行数に基づいてイテレーション。tqdmで進捗を表示。\n",
    "    # unit=\"sequences\" は進捗バーの単位がシーケンスであることを示す。\n",
    "    # leave=True (デフォルト) はループ終了後もプログレスバーを残す。\n",
    "    for t in tqdm(range(SEQ_LEN-1, len(df)-6), desc=description, unit=\"sequences\", leave=True):\n",
    "        win = df.iloc[t-SEQ_LEN+1:t+1]\n",
    "        \n",
    "        # --- 特徴量生成 ---\n",
    "        c0 = win['close'].iloc[-1] \n",
    "        v0 = win['volume'].iloc[-1]\n",
    "\n",
    "        norm_close = win['close'] / c0\n",
    "        norm_high = win['high'] / c0\n",
    "        norm_low = win['low'] / c0\n",
    "        \n",
    "        if v0 != 0:\n",
    "            normalized_volume = win['volume'] / v0\n",
    "        else:\n",
    "            normalized_volume = np.zeros_like(win['volume'], dtype=float) \n",
    "        \n",
    "        feats = np.stack([norm_close,\n",
    "                          norm_high,\n",
    "                          norm_low,\n",
    "                          normalized_volume], axis=1)\n",
    "        X.append(feats)\n",
    "        \n",
    "        # --- ラベル生成 ---\n",
    "        y_hard.append(hard_lbl[t])\n",
    "        y_soft.append(soft_lbl[t])\n",
    "        \n",
    "        if t + 1 < len(df): \n",
    "            c_plus_1 = df['close'].iloc[t+1]\n",
    "            if c0 != 0:\n",
    "                reg_target = c_plus_1 / c0\n",
    "            else:\n",
    "                reg_target = 1.0 \n",
    "        else:\n",
    "            reg_target = 1.0\n",
    "        y_reg.append(reg_target)\n",
    "        \n",
    "        t_indices.append(t) \n",
    "        \n",
    "    return np.array(X, dtype=np.float32), \\\n",
    "           np.array(y_hard), \\\n",
    "           np.array(y_soft, dtype=np.float32), \\\n",
    "           np.array(y_reg, dtype=np.float32), \\\n",
    "           np.array(t_indices)\n",
    "\n",
    "# --- データセット作成実行 ---\n",
    "# make_dataset呼び出し時に、tqdmプログレスバー用の説明文を指定\n",
    "print(\"--- Training data creation ---\")\n",
    "X_tr, y_tr_hard, y_tr_soft, y_tr_reg, t_tr_indices = make_dataset(\n",
    "    df_train, hard_train, soft_train, description=\"Processing df_train\"\n",
    ")\n",
    "print(\"\\n--- Validation/Test data creation ---\")\n",
    "X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices = make_dataset(\n",
    "    df_valid, hard_val, soft_val, description=\"Processing df_valid\"\n",
    ")\n",
    "print(\"\") # tqdmの表示とprint文の間にスペースを空ける\n",
    "\n",
    "# --- 検証データとテストデータの分割 ---\n",
    "if len(X_val_all) > 1:\n",
    "    X_val, X_test, \\\n",
    "    y_val_hard, y_test_hard, \\\n",
    "    y_val_soft, y_test_soft, \\\n",
    "    y_val_reg, y_test_reg, \\\n",
    "    t_val_indices, t_test_indices = train_test_split(\n",
    "        X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices,\n",
    "        test_size=0.5, random_state=SEED, shuffle=True # SEEDは事前に定義されている前提\n",
    "    )\n",
    "else:\n",
    "    print(\"Warning: Not enough data in df_valid to split into validation and test sets after make_dataset. Using all for validation and test if applicable.\")\n",
    "    X_val, y_val_hard, y_val_soft, y_val_reg, t_val_indices = X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices\n",
    "    X_test, y_test_hard, y_test_soft, y_test_reg, t_test_indices = X_val_all, y_val_hard_all, y_val_soft_all, y_val_reg_all, t_val_all_indices\n",
    "\n",
    "# --- データ形状の確認表示 ---\n",
    "print(f\"X_tr shape: {X_tr.shape}, y_tr_reg shape: {y_tr_reg.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape if len(X_val)>0 else 'N/A'}, y_val_reg shape: {y_val_reg.shape if len(X_val)>0 else 'N/A'}\")\n",
    "print(f\"X_test shape: {X_test.shape if len(X_test)>0 else 'N/A'}, y_test_reg shape: {y_test_reg.shape if len(X_test)>0 else 'N/A'}\")\n",
    "\n",
    "# Step 8: Dataset & DataLoader\n",
    "class BTCSeqDataset(Dataset):\n",
    "    def __init__(self, X, y_hard, y_soft, y_reg, t_indices): # y_reg を追加\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y_hard = torch.from_numpy(y_hard)\n",
    "        self.y_soft = torch.from_numpy(y_soft)\n",
    "        self.y_reg = torch.from_numpy(y_reg) # y_reg を torch tensor に変換\n",
    "        self.t_indices = torch.from_numpy(t_indices)\n",
    "\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y_hard[i], self.y_soft[i], self.y_reg[i], self.t_indices[i] # y_reg を返す\n",
    "\n",
    "BATCH = 64\n",
    "dl_tr = DataLoader(BTCSeqDataset(X_tr, y_tr_hard, y_tr_soft, y_tr_reg, t_tr_indices), batch_size=BATCH, shuffle=True)\n",
    "\n",
    "if len(X_val) > 0 :\n",
    "    dl_val = DataLoader(BTCSeqDataset(X_val, y_val_hard, y_val_soft, y_val_reg, t_val_indices), batch_size=BATCH)\n",
    "else:\n",
    "    dl_val = None\n",
    "    print(\"Validation DataLoader (dl_val) is not created due to empty X_val.\")\n",
    "\n",
    "if len(X_test) > 0:\n",
    "    dl_test= DataLoader(BTCSeqDataset(X_test, y_test_hard, y_test_soft, y_test_reg, t_test_indices), batch_size=BATCH)\n",
    "else:\n",
    "    dl_test = None\n",
    "    print(\"Test DataLoader (dl_test) is not created due to empty X_test.\")\n",
    "    \n",
    "# Step 9: PositionalEncoding + TransformerClassifier with SoftLabel Loss\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000)/d_model))\n",
    "        pe[:,0::2] = torch.sin(pos * div)\n",
    "        pe[:,1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))\n",
    "    def forward(self, x): return x + self.pe[:x.size(0)]\n",
    "\n",
    "class TransformerClassifierSoftLabel(nn.Module):\n",
    "    def __init__(self, d_model=120, nhead=3, num_layers=2, num_classes=3, lambda_reg=0.1):\n",
    "        super().__init__()\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.proj = nn.Linear(4, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        layer = nn.TransformerEncoderLayer(d_model, nhead=nhead, dim_feedforward=256)\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_classes))\n",
    "        self.regressor  = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.proj(x).permute(1,0,2)\n",
    "        h = self.pos_enc(h)\n",
    "        h = self.encoder(h)\n",
    "        cls = h[-1]\n",
    "        return self.classifier(cls), self.regressor(cls).squeeze(-1)\n",
    "\n",
    "    def compute_loss(self, logits, y_hard, y_soft, reg, y_reg):\n",
    "        # 分類損失: KLDiv between softmax(logits) & soft labels\n",
    "        loss_cls = F.kl_div(F.log_softmax(logits, dim=-1), y_soft, reduction='batchmean')\n",
    "        # 回帰損失: MSE\n",
    "        loss_reg = F.mse_loss(reg, y_reg)\n",
    "        loss = loss_cls + self.lambda_reg * loss_reg\n",
    "        return loss, loss_cls.item(), loss_reg.item()\n",
    "\n",
    "model = TransformerClassifierSoftLabel().to(device)\n",
    "opt = Adam(model.parameters(), lr=1e-4)\n",
    "sched = CosineAnnealingLR(opt, T_max=10)\n",
    "\n",
    "# 指標出力のためのヘルパー関数\n",
    "\n",
    "# --- 必要なライブラリ (スクリプトの先頭で一度インポートすることを推奨) ---\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, precision_recall_curve, auc, f1_score\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt # pltをインポート\n",
    "from tqdm.auto import tqdm # tqdm.auto は環境に応じて適切な tqdm を選択\n",
    "import pandas as pd # pd.isna のためにインポート\n",
    "\n",
    "# --- Helper Function: 閾値ベースの3クラス予測ラベル生成 ---\n",
    "def get_thresholded_predictions_final(probs_np, threshold_val):\n",
    "    y_pred_thresh = np.zeros(len(probs_np), dtype=int) \n",
    "    prob_min_scores = probs_np[:, 1] \n",
    "    prob_max_scores = probs_np[:, 2] \n",
    "\n",
    "    for i in range(len(probs_np)):\n",
    "        is_min_candidate = prob_min_scores[i] >= threshold_val\n",
    "        is_max_candidate = prob_max_scores[i] >= threshold_val\n",
    "\n",
    "        if is_min_candidate and is_max_candidate:\n",
    "            if prob_min_scores[i] >= prob_max_scores[i]:\n",
    "                y_pred_thresh[i] = 1 \n",
    "            else:\n",
    "                y_pred_thresh[i] = 2 \n",
    "        elif is_min_candidate:\n",
    "            y_pred_thresh[i] = 1 \n",
    "        elif is_max_candidate:\n",
    "            y_pred_thresh[i] = 2 \n",
    "    return y_pred_thresh\n",
    "\n",
    "# --- Helper Function: 将来変動率の統計計算とファイル書き込み (最新の修正を反映) ---\n",
    "def log_future_price_change_stats_revised(f_returns_file, y_predictions_at_threshold, \n",
    "                                          original_indices_np, source_df,\n",
    "                                          class_index_to_analyze, class_name_str, \n",
    "                                          num_future_steps, current_threshold_val):\n",
    "    changes_to_low_list = []\n",
    "    changes_to_high_list = []\n",
    "    \n",
    "    predicted_as_class_indices = np.where(y_predictions_at_threshold == class_index_to_analyze)[0]\n",
    "\n",
    "    f_returns_file.write(f\"Class: {class_name_str} (Predictions based on Threshold: {current_threshold_val:.2f})\\n\")\n",
    "\n",
    "    if len(predicted_as_class_indices) == 0:\n",
    "        f_returns_file.write(f\"  No samples predicted as {class_name_str}.\\n\")\n",
    "        f_returns_file.write(f\"  Future LOW price change (%): Mean=N/A, Median=N/A, Q1=N/A, Q3=N/A\\n\")\n",
    "        f_returns_file.write(f\"  Future HIGH price change (%): Mean=N/A, Median=N/A, Q1=N/A, Q3=N/A\\n\\n\")\n",
    "        return\n",
    "\n",
    "    for arr_idx in predicted_as_class_indices:\n",
    "        original_df_idx = original_indices_np[arr_idx]\n",
    "        if not (0 <= original_df_idx < len(source_df)): continue \n",
    "            \n",
    "        current_data_point = source_df.iloc[original_df_idx]\n",
    "        current_close = current_data_point['close']\n",
    "        \n",
    "        future_start_idx = original_df_idx + 1\n",
    "        future_end_idx = original_df_idx + 1 + num_future_steps\n",
    "\n",
    "        if future_end_idx <= len(source_df):\n",
    "            future_window_df = source_df.iloc[future_start_idx:future_end_idx]\n",
    "            if future_window_df.empty: \n",
    "                changes_to_low_list.append(np.nan)\n",
    "                changes_to_high_list.append(np.nan)\n",
    "                continue\n",
    "\n",
    "            future_low_price = np.nan\n",
    "            future_high_price = np.nan\n",
    "            if not future_window_df['low'].empty:\n",
    "                future_low_price = future_window_df['low'].min()\n",
    "            if not future_window_df['high'].empty:\n",
    "                future_high_price = future_window_df['high'].max()\n",
    "            \n",
    "            # Pct change to future low\n",
    "            if pd.isna(future_low_price) or pd.isna(current_close) or current_close == 0:\n",
    "                changes_to_low_list.append(np.nan)\n",
    "            else:\n",
    "                pct_change_low = ((future_low_price - current_close) / current_close) * 100\n",
    "                changes_to_low_list.append(pct_change_low)\n",
    "\n",
    "            # Pct change to future high\n",
    "            if pd.isna(future_high_price) or pd.isna(current_close) or current_close == 0:\n",
    "                changes_to_high_list.append(np.nan)\n",
    "            else:\n",
    "                pct_change_high = ((future_high_price - current_close) / current_close) * 100\n",
    "                changes_to_high_list.append(pct_change_high)\n",
    "        else: \n",
    "            changes_to_low_list.append(np.nan)\n",
    "            changes_to_high_list.append(np.nan)\n",
    "\n",
    "    # --- Stats for future low price changes ---\n",
    "    changes_to_low_cleaned = [c for c in changes_to_low_list if not pd.isna(c)]\n",
    "    f_returns_file.write(f\"  For {class_name_str} predictions (count: {len(predicted_as_class_indices)}, valid for low: {len(changes_to_low_cleaned)}):\\n\")\n",
    "    if changes_to_low_cleaned:\n",
    "        mean_change = np.mean(changes_to_low_cleaned)\n",
    "        median_change = np.median(changes_to_low_cleaned)\n",
    "        q1_change = np.percentile(changes_to_low_cleaned, 25)\n",
    "        q3_change = np.percentile(changes_to_low_cleaned, 75)\n",
    "        f_returns_file.write(f\"  Future LOW price change (%): Mean={mean_change:.4f}, Median={median_change:.4f}, Q1={q1_change:.4f}, Q3={q3_change:.4f}\\n\")\n",
    "    else:\n",
    "        f_returns_file.write(f\"  Future LOW price change (%): Mean=N/A, Median=N/A, Q1=N/A, Q3=N/A\\n\")\n",
    "\n",
    "    # --- Stats for future high price changes ---\n",
    "    changes_to_high_cleaned = [c for c in changes_to_high_list if not pd.isna(c)]\n",
    "    # Count information is already printed above for this class prediction\n",
    "    if changes_to_high_cleaned: \n",
    "        mean_change = np.mean(changes_to_high_cleaned)\n",
    "        median_change = np.median(changes_to_high_cleaned)\n",
    "        q1_change = np.percentile(changes_to_high_cleaned, 25)\n",
    "        q3_change = np.percentile(changes_to_high_cleaned, 75)\n",
    "        f_returns_file.write(f\"  Future HIGH price change (%): Mean={mean_change:.4f}, Median={median_change:.4f}, Q1={q1_change:.4f}, Q3={q3_change:.4f}\\n\\n\")\n",
    "    else:\n",
    "        f_returns_file.write(f\"  Future HIGH price change (%): Mean=N/A, Median=N/A, Q1=N/A, Q3=N/A\\n\\n\")\n",
    "\n",
    "# --- Main Evaluation Function (テストデータ評価で使用) ---\n",
    "def perform_detailed_evaluation(model_instance, dataloader_to_eval, original_df_for_lookup,\n",
    "                                device_to_use, output_path, file_prefix_str,\n",
    "                                class_target_names, prob_thresholds, num_future_steps,\n",
    "                                epoch_info_str=\"N/A\"):\n",
    "    # (この関数の実装は、前回の100点回答で提示した `perform_detailed_evaluation` と同じです)\n",
    "    # 内部で get_thresholded_predictions_final と log_future_price_change_stats_revised を使用します。\n",
    "    print(f\"\\n--- Starting Detailed Evaluation for: {file_prefix_str} (Source Epoch for Model: {epoch_info_str}) ---\")\n",
    "    if dataloader_to_eval is None:\n",
    "        print(f\"ERROR: DataLoader for '{file_prefix_str}' is None. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    model_instance.eval() \n",
    "    all_true_labels, all_probs, all_original_indices = [], [], []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        eval_pbar = tqdm(dataloader_to_eval, desc=f\"Evaluating {file_prefix_str}\", leave=False)\n",
    "        for Xb, yh_hard, _, _, t_idx in eval_pbar: \n",
    "            Xb = Xb.to(device_to_use)\n",
    "            logits, _ = model_instance(Xb) \n",
    "            \n",
    "            all_probs.append(torch.softmax(logits, dim=-1).cpu().numpy())\n",
    "            all_true_labels.append(yh_hard.cpu().numpy())\n",
    "            all_original_indices.append(t_idx.cpu().numpy())\n",
    "\n",
    "    if not all_true_labels: \n",
    "        print(f\"ERROR: No data processed from DataLoader for '{file_prefix_str}'. Skipping evaluation.\")\n",
    "        return\n",
    "\n",
    "    y_true_np = np.concatenate(all_true_labels)\n",
    "    probs_np = np.concatenate(all_probs)\n",
    "    original_indices_np = np.concatenate(all_original_indices)\n",
    "\n",
    "    if not (len(y_true_np) == len(probs_np) == len(original_indices_np)):\n",
    "        print(f\"ERROR: Mismatch in resulted array lengths for {file_prefix_str}. Aborting metrics calculation.\")\n",
    "        return\n",
    "\n",
    "    ep_info_for_filename = str(epoch_info_str).replace(\" \", \"_\") \n",
    "    metrics_filepath = os.path.join(output_path, f\"{file_prefix_str}_metrics_ep{ep_info_for_filename}.txt\")\n",
    "    returns_filepath = os.path.join(output_path, f\"{file_prefix_str}_future_returns_ep{ep_info_for_filename}.txt\")\n",
    "\n",
    "    with open(metrics_filepath, \"w\") as f_metrics, open(returns_filepath, \"w\") as f_returns:\n",
    "        f_metrics.write(f\"Detailed Metrics for {file_prefix_str} (Model from Epoch: {epoch_info_str})\\n\")\n",
    "        f_returns.write(f\"Future Returns Analysis for {file_prefix_str} (Model from Epoch: {epoch_info_str})\\n\")\n",
    "\n",
    "        f_metrics.write(\"=\"*50 + \"\\nOverall Class-Specific AUCs (Threshold-Independent):\\n\")\n",
    "        for class_idx, class_name in enumerate(class_target_names): # class_target_names は target_names\n",
    "            if class_idx == 0: continue \n",
    "            if np.any(y_true_np == class_idx) and np.any(y_true_np != class_idx):\n",
    "                try:\n",
    "                    roc_auc = roc_auc_score((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                    precision, recall, _ = precision_recall_curve((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    f_metrics.write(f\"  {class_name} - ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\\n\")\n",
    "                except ValueError as e_auc:\n",
    "                    f_metrics.write(f\"  {class_name} - ROC-AUC: Error ({e_auc}), PR-AUC: Error\\n\") \n",
    "            else:\n",
    "                f_metrics.write(f\"  {class_name} - ROC-AUC: N/A, PR-AUC: N/A (Insufficient class diversity or no samples for this class)\\n\")\n",
    "        f_metrics.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "        for th in prob_thresholds: # prob_thresholds は thresholds\n",
    "            f_metrics.write(f\"--- Metrics for Threshold: {th:.2f} ---\\n\")\n",
    "            f_returns.write(f\"--- Returns Analysis for Threshold: {th:.2f} ---\\n\")\n",
    "\n",
    "            y_pred_at_threshold = get_thresholded_predictions_final(probs_np, th)\n",
    "            \n",
    "            cm = confusion_matrix(y_true_np, y_pred_at_threshold, labels=[0,1,2])\n",
    "            f_metrics.write(\"Confusion Matrix:\\n\" + np.array2string(cm) + \"\\n\\n\")\n",
    "            try:\n",
    "                cr = classification_report(y_true_np, y_pred_at_threshold, target_names=class_target_names, labels=[0,1,2], zero_division=0, digits=4)\n",
    "                f_metrics.write(\"Classification Report:\\n\" + cr + \"\\n\\n\")\n",
    "            except Exception as e_cr:\n",
    "                f_metrics.write(f\"Could not generate Classification Report for threshold {th:.2f}: {e_cr}\\n\\n\")\n",
    "\n",
    "            f1_min = f1_score((y_true_np == 1).astype(int), (probs_np[:, 1] > th).astype(int), zero_division=0)\n",
    "            f1_max = f1_score((y_true_np == 2).astype(int), (probs_np[:, 2] > th).astype(int), zero_division=0)\n",
    "            f_metrics.write(f\"Binary F1-score (Min vs Rest) @{th:.2f}: {f1_min:.4f}\\n\")\n",
    "            f_metrics.write(f\"Binary F1-score (Max vs Rest) @{th:.2f}: {f1_max:.4f}\\n\")\n",
    "            f_metrics.write(\"-\" * 40 + \"\\n\\n\")\n",
    "            \n",
    "            log_future_price_change_stats_revised(f_returns, y_pred_at_threshold, original_indices_np, original_df_for_lookup,\n",
    "                                                  1, class_target_names[1], num_future_steps, th) # num_future_steps は FUTURE_STEPS\n",
    "            log_future_price_change_stats_revised(f_returns, y_pred_at_threshold, original_indices_np, original_df_for_lookup,\n",
    "                                                  2, class_target_names[2], num_future_steps, th)\n",
    "    \n",
    "    print(f\"INFO: Detailed metrics for '{file_prefix_str}' saved to: {metrics_filepath}\")\n",
    "    print(f\"INFO: Future returns analysis for '{file_prefix_str}' saved to: {returns_filepath}\")\n",
    "    print(f\"--- Finished Detailed Evaluation for: {file_prefix_str} ---\")\n",
    "    \n",
    "# Step 10: 学習ループ (tqdm + ライブプロット + 詳細評価 + ベストモデル保存)\n",
    "\n",
    "# --- このStep10のコードブロックを実行するための前提条件 ---\n",
    "# 以下のオブジェクト・変数は、これより前のStep (1-9) で定義・初期化されている必要があります:\n",
    "#   model, opt, sched, dl_tr, dl_val, df_valid, device, output_dir\n",
    "#   TransformerClassifierSoftLabel クラス定義\n",
    "#   ヘルパー関数: get_thresholded_predictions_final, log_future_price_change_stats_revised\n",
    "# 必要なライブラリ: torch, numpy as np, os, sklearn.metrics 各種, \n",
    "#                 IPython.display (display, clear_output), matplotlib.pyplot as plt, tqdm.auto\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(f\"INFO: Step 10 - Initializing variables and starting training loop.\")\n",
    "\n",
    "# --- Step10で必要となるパラメータと状態変数の定義・初期化 ---\n",
    "EPOCHS = 30  # 例: 総エポック数 (実際の値に合わせてください)\n",
    "thresholds = [0.25, 0.5, 0.75] # 評価に使用する確率閾値のリスト\n",
    "target_names = ['Other', 'Min', 'Max'] # クラスラベル名\n",
    "FUTURE_STEPS = 6 # 将来の価格変動を何ステップ先まで見るか\n",
    "\n",
    "# 学習履歴保存用\n",
    "history = {'train_total':[], 'train_cls':[], 'train_reg':[],\n",
    "           'val_total':[],   'val_cls':[],   'val_reg':[]}\n",
    "\n",
    "# ベストモデル保存用\n",
    "min_val_loss = float('inf')\n",
    "# output_dir は Step7 で定義されている想定\n",
    "best_model_path = os.path.join(output_dir, \"best_transformer_model.pth\") \n",
    "best_epoch = -1\n",
    "\n",
    "# ライブプロット用の図と軸を準備 (再実行時のために存在確認と初期化)\n",
    "# この fig_loss, ax_loss はこのStep10のブロック内で閉じることを推奨 (plt.close(fig_loss))\n",
    "# または、上位のスコープで管理し、ここではその存在を前提とする。\n",
    "# 今回は、このStep10内で完結するように初期化。\n",
    "if 'fig_loss' not in locals() or fig_loss is None or not plt.fignum_exists(fig_loss.number):\n",
    "    fig_loss, ax_loss = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    print(\"INFO: Step 10 - Loss plot figure initialized.\")\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "print(f\"INFO: Starting training and evaluation loop for {EPOCHS} epochs.\")\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    # --- Train Phase ---\n",
    "    model.train()\n",
    "    train_run_metrics = {'total_loss': 0.0, 'cls_loss': 0.0, 'reg_loss': 0.0}\n",
    "    num_train_batches = 0\n",
    "    train_pbar = tqdm(dl_tr, desc=f\"Epoch {ep}/{EPOCHS} [Train]\", leave=False)\n",
    "    for Xb, yh_hard, ys_soft, y_reg_target, _ in train_pbar: # t_indices は訓練では未使用\n",
    "        Xb, yh_hard, ys_soft, y_reg_target = Xb.to(device), yh_hard.to(device), ys_soft.to(device), y_reg_target.to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        logits, reg_pred = model(Xb)\n",
    "        loss, lcls, lreg = model.compute_loss(logits, yh_hard, ys_soft, reg_pred, y_reg_target)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_run_metrics['total_loss'] += loss.item()\n",
    "        train_run_metrics['cls_loss'] += lcls # .item() は compute_loss 内で行われている前提\n",
    "        train_run_metrics['reg_loss'] += lreg # .item() は compute_loss 内で行われている前提\n",
    "        num_train_batches += 1\n",
    "        train_pbar.set_postfix({k_train: v_train / num_train_batches for k_train, v_train in train_run_metrics.items()})\n",
    "\n",
    "    if hasattr(sched, 'step'): # スケジューラが存在し、stepメソッドを持つ場合\n",
    "        sched.step()\n",
    "    \n",
    "    history['train_total'].append(train_run_metrics['total_loss'] / num_train_batches if num_train_batches > 0 else 0)\n",
    "    history['train_cls'].append(train_run_metrics['cls_loss'] / num_train_batches if num_train_batches > 0 else 0)\n",
    "    history['train_reg'].append(train_run_metrics['reg_loss'] / num_train_batches if num_train_batches > 0 else 0)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    if dl_val: # dl_val が None でないことを確認\n",
    "        model.eval()\n",
    "        val_run_metrics = {'total_loss': 0.0, 'cls_loss': 0.0, 'reg_loss': 0.0}\n",
    "        num_val_batches = 0\n",
    "        y_true_val_hard_all, probs_val_all, t_indices_val_all = [], [], []\n",
    "        \n",
    "        val_pbar = tqdm(dl_val, desc=f\"Epoch {ep}/{EPOCHS} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for Xb_val, yh_hard_val, ys_soft_val, y_reg_target_val, t_idx_val in val_pbar:\n",
    "                Xb_val, yh_hard_val, ys_soft_val, y_reg_target_val = Xb_val.to(device), yh_hard_val.to(device), ys_soft_val.to(device), y_reg_target_val.to(device)\n",
    "                \n",
    "                logits_val, reg_pred_val = model(Xb_val)\n",
    "                loss_val, lcls_val, lreg_val = model.compute_loss(logits_val, yh_hard_val, ys_soft_val, reg_pred_val, y_reg_target_val)\n",
    "                \n",
    "                val_run_metrics['total_loss'] += loss_val.item()\n",
    "                val_run_metrics['cls_loss'] += lcls_val\n",
    "                val_run_metrics['reg_loss'] += lreg_val\n",
    "                num_val_batches += 1\n",
    "                val_pbar.set_postfix({k_val: v_val / num_val_batches for k_val, v_val in val_run_metrics.items()})\n",
    "\n",
    "                probs_val_all.append(torch.softmax(logits_val, dim=-1).cpu().numpy())\n",
    "                y_true_val_hard_all.append(yh_hard_val.cpu().numpy())\n",
    "                t_indices_val_all.append(t_idx_val.cpu().numpy())\n",
    "\n",
    "        current_epoch_val_total_loss = val_run_metrics['total_loss'] / num_val_batches if num_val_batches > 0 else float('inf')\n",
    "        history['val_total'].append(current_epoch_val_total_loss)\n",
    "        history['val_cls'].append(val_run_metrics['cls_loss'] / num_val_batches if num_val_batches > 0 else 0)\n",
    "        history['val_reg'].append(val_run_metrics['reg_loss'] / num_val_batches if num_val_batches > 0 else 0)\n",
    "\n",
    "        if current_epoch_val_total_loss < min_val_loss:\n",
    "            min_val_loss = current_epoch_val_total_loss\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            best_epoch = ep\n",
    "            # tqdmを使っている場合、printがバーを乱すことがあるため、ループ外での最終報告を推奨\n",
    "            # print(f\"\\nEpoch {ep}: New best model saved! Val Total Loss: {min_val_loss:.4f}\") \n",
    "        \n",
    "        # --- Validation Metrics & Future Returns Analysis ---\n",
    "        if num_val_batches > 0:\n",
    "            y_true_np = np.concatenate(y_true_val_hard_all)\n",
    "            probs_np = np.concatenate(probs_val_all)\n",
    "            val_original_indices_np = np.concatenate(t_indices_val_all)\n",
    "\n",
    "            # ファイル名はエポック番号を3桁ゼロ埋めにするなど工夫するとソートしやすい\n",
    "            metrics_filepath = os.path.join(output_dir, f\"validation_metrics_ep{ep:03d}.txt\")\n",
    "            returns_filepath = os.path.join(output_dir, f\"validation_future_returns_ep{ep:03d}.txt\")\n",
    "\n",
    "            with open(metrics_filepath, \"w\") as f_metrics, open(returns_filepath, \"w\") as f_returns:\n",
    "                f_metrics.write(f\"Epoch {ep} - Validation Set Metrics\\n\")\n",
    "                f_returns.write(f\"Epoch {ep} - Validation Set Future Returns Analysis\\n\")\n",
    "                \n",
    "                f_metrics.write(\"=\"*50 + \"\\nOverall Class-Specific AUCs (Threshold-Independent):\\n\")\n",
    "                for class_idx, class_name in enumerate(target_names):\n",
    "                    if class_idx == 0: continue # Skip 'Other' class for this specific AUC reporting\n",
    "                    if np.any(y_true_np == class_idx) and np.any(y_true_np != class_idx):\n",
    "                        try:\n",
    "                            roc_auc = roc_auc_score((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                            precision, recall, _ = precision_recall_curve((y_true_np == class_idx).astype(int), probs_np[:, class_idx])\n",
    "                            pr_auc = auc(recall, precision)\n",
    "                            f_metrics.write(f\"  {class_name} - ROC-AUC: {roc_auc:.4f}, PR-AUC: {pr_auc:.4f}\\n\")\n",
    "                        except ValueError as e_auc: # ハンドリングを追加\n",
    "                             f_metrics.write(f\"  {class_name} - ROC-AUC: Error ({e_auc}), PR-AUC: Error\\n\")\n",
    "                    else:\n",
    "                        f_metrics.write(f\"  {class_name} - ROC-AUC: N/A, PR-AUC: N/A (Insufficient class diversity or no samples for this class)\\n\")\n",
    "                f_metrics.write(\"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "                for th in thresholds:\n",
    "                    f_metrics.write(f\"--- Metrics for Threshold: {th:.2f} ---\\n\")\n",
    "                    f_returns.write(f\"--- Returns Analysis for Threshold: {th:.2f} ---\\n\")\n",
    "\n",
    "                    # get_thresholded_predictions_final は事前に定義されている前提\n",
    "                    y_pred_at_threshold = get_thresholded_predictions_final(probs_np, th)\n",
    "                    \n",
    "                    cm = confusion_matrix(y_true_np, y_pred_at_threshold, labels=[0,1,2])\n",
    "                    f_metrics.write(\"Confusion Matrix:\\n\" + np.array2string(cm) + \"\\n\\n\")\n",
    "                    try:\n",
    "                        cr = classification_report(y_true_np, y_pred_at_threshold, target_names=target_names, labels=[0,1,2], zero_division=0, digits=4)\n",
    "                        f_metrics.write(\"Classification Report:\\n\" + cr + \"\\n\\n\")\n",
    "                    except Exception as e_cr: \n",
    "                        f_metrics.write(f\"Could not generate Classification Report for threshold {th:.2f}: {e_cr}\\n\\n\")\n",
    "\n",
    "                    f1_min = f1_score((y_true_np == 1).astype(int), (probs_np[:, 1] > th).astype(int), zero_division=0)\n",
    "                    f1_max = f1_score((y_true_np == 2).astype(int), (probs_np[:, 2] > th).astype(int), zero_division=0)\n",
    "                    f_metrics.write(f\"Binary F1-score (Min vs Rest) @{th:.2f}: {f1_min:.4f}\\n\")\n",
    "                    f_metrics.write(f\"Binary F1-score (Max vs Rest) @{th:.2f}: {f1_max:.4f}\\n\")\n",
    "                    f_metrics.write(\"-\" * 40 + \"\\n\\n\")\n",
    "                    \n",
    "                    # log_future_price_change_stats_revised は事前に定義されている前提\n",
    "                    # df_valid は上位スコープから参照\n",
    "                    log_future_price_change_stats_revised(f_returns, y_pred_at_threshold, val_original_indices_np, df_valid,\n",
    "                                                          1, target_names[1], FUTURE_STEPS, th)\n",
    "                    log_future_price_change_stats_revised(f_returns, y_pred_at_threshold, val_original_indices_np, df_valid,\n",
    "                                                          2, target_names[2], FUTURE_STEPS, th)\n",
    "            \n",
    "            # tqdmを使っている場合、printがバーを乱すことがあるため、ループ外での最終報告を推奨\n",
    "            # if ep < EPOCHS: print() # ループの最後以外で改行を入れるなど工夫\n",
    "    else: # dl_val がない場合\n",
    "        history['val_total'].append(float('inf')) # val_lossは無限大として扱う\n",
    "        history['val_cls'].append(0)\n",
    "        history['val_reg'].append(0)\n",
    "\n",
    "    # --- Plot Losses ---\n",
    "    clear_output(wait=True) # Jupyter Notebook/Lab環境でプロットを更新表示\n",
    "    for i, key_suffix in enumerate(['total', 'cls', 'reg']):\n",
    "        ax = ax_loss[i] # ax_loss[i] を ax に代入\n",
    "        ax.clear()\n",
    "        ax.plot(history[f'train_{key_suffix}'], label=f'train_{key_suffix}')\n",
    "        # valのロスが記録されているか、かつinfでない場合のみプロット\n",
    "        if dl_val and len(history[f'val_{key_suffix}']) > 0 and not np.all(np.isinf(history[f'val_{key_suffix}'])):\n",
    "             ax.plot(history[f'val_{key_suffix}'], label=f'val_{key_suffix}')\n",
    "        ax.set_title(f\"{key_suffix.capitalize()} Loss (Epoch {ep})\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Average Loss\")\n",
    "        ax.legend()\n",
    "    fig_loss.tight_layout()\n",
    "    display(fig_loss) # Jupyter Notebook/Lab環境で図を表示\n",
    "\n",
    "# --- End of Training Loop ---\n",
    "print(\"\\nINFO: Step 10 - Training Loop Finished ---\")\n",
    "if best_epoch != -1:\n",
    "    print(f\"Best model found at epoch {best_epoch} with validation total loss: {min_val_loss:.4f}\")\n",
    "    print(f\"Best model weights saved to: {best_model_path}\")\n",
    "else:\n",
    "    print(\"No best model was saved (either validation loss did not improve or no validation data was provided).\")\n",
    "\n",
    "# 学習ループ終了後にプロットウィンドウを閉じる場合は以下を有効化\n",
    "# plt.close(fig_loss)\n",
    "\n",
    "# Step 11: テスト評価 (詳細評価)\n",
    "\n",
    "# --- このStep11のコードブロックを実行するための前提条件 ---\n",
    "# (上記のヘルパー関数定義部分のコメント、およびStep10の前提条件リストを参照)\n",
    "# 特に、TransformerClassifierSoftLabel クラス定義、model (最終エポックモデル)、dl_test, \n",
    "# df_valid (またはテストデータに対応する適切なDataFrame), device, output_dir, \n",
    "# EPOCHS, thresholds, target_names, FUTURE_STEPS, best_model_path, best_epoch\n",
    "# が定義されている必要があります。\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "print(\"\\nINFO: Step 11 - Starting Test Set Evaluation ---\")\n",
    "\n",
    "# テスト用の新しいモデルインスタンス。学習時に使用したクラスと同じ定義であること。\n",
    "# (TransformerClassifierSoftLabelクラスの定義がこのスコープで利用可能であること)\n",
    "model_for_testing = TransformerClassifierSoftLabel().to(device) \n",
    "\n",
    "if dl_test is not None:\n",
    "    loaded_best_model = False\n",
    "    if os.path.exists(best_model_path) and best_epoch != -1:\n",
    "        print(f\"INFO: Loading best model from epoch {best_epoch} (Path: {best_model_path}) for test evaluation.\")\n",
    "        try:\n",
    "            model_for_testing.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "            loaded_best_model = True\n",
    "            perform_detailed_evaluation(model_instance=model_for_testing,\n",
    "                                        dataloader_to_eval=dl_test,\n",
    "                                        original_df_for_lookup=df_valid, # ★重要: dl_testの元データがdf_validでない場合は適切なDFを指定\n",
    "                                        device_to_use=device,\n",
    "                                        output_path=output_dir,\n",
    "                                        file_prefix_str=\"test_best_model\",\n",
    "                                        class_target_names=target_names,\n",
    "                                        prob_thresholds=thresholds,\n",
    "                                        num_future_steps=FUTURE_STEPS,\n",
    "                                        epoch_info_str=str(best_epoch))\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Failed to load or evaluate best model: {e}\")\n",
    "            loaded_best_model = False # Mark as not loaded if error occurs\n",
    "    \n",
    "    # ベストモデルのロード/評価に失敗した場合、またはベストモデルが存在しない場合に最終エポックモデルで評価\n",
    "    if not loaded_best_model:\n",
    "        if 'model' in locals() and model is not None: # 学習ループ後の'model'が存在するか確認\n",
    "            print(f\"INFO: Best model not loaded or evaluation failed. Evaluating test set with the model from the last training epoch ({EPOCHS}).\")\n",
    "            perform_detailed_evaluation(model_instance=model, # model は学習ループ後の最終状態\n",
    "                                        dataloader_to_eval=dl_test,\n",
    "                                        original_df_for_lookup=df_valid, # ★重要: dl_testの元データがdf_validでない場合は適切なDFを指定\n",
    "                                        device_to_use=device,\n",
    "                                        output_path=output_dir,\n",
    "                                        file_prefix_str=\"test_last_epoch_model\",\n",
    "                                        class_target_names=target_names,\n",
    "                                        prob_thresholds=thresholds,\n",
    "                                        num_future_steps=FUTURE_STEPS,\n",
    "                                        epoch_info_str=str(EPOCHS))\n",
    "        else:\n",
    "            print(\"ERROR: No model available (neither best loaded nor last epoch 'model' found) for test evaluation.\")\n",
    "else:\n",
    "    print(\"INFO: Test data loader (dl_test) is None. Skipping test evaluation.\")\n",
    "\n",
    "print(\"INFO: Step 11 - Test Evaluation Phase Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
